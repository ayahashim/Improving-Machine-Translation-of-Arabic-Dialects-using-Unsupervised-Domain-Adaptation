# -*- coding: utf-8 -*-
"""Transformer_Domain_Adaptation_model_Final_with_LORA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZmwwBxAUg2PP6xI7Uy8bUTzZNM5VTd8
"""

# !pip install transformers

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive"

# -*- coding: utf-8 -*-
"""Transformer_batch_size_arabert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKjYDLWxlvcf-zuNLSXj0c8JgvfOD2ey
"""

from io import open
import unicodedata
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from transformers import ( AutoModel, AutoTokenizer)
import numpy as np
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from torch import Tensor
import torch
import torch.nn as nn
from torch.nn import Transformer
import math
from timeit import default_timer as timer
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CosineAnnealingLR

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import pandas as pd
from sklearn.model_selection import train_test_split
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "aubmindlab/bert-base-arabertv02-twitter"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_name)
# data_vocab = './data_all'
# data_file = "./src_train_GULF"
# data_trgt_file = './trgt_train_GULF'
SOS_token = tokenizer.cls_token_id
EOS_token = tokenizer.sep_token_id
PAD_token = tokenizer.pad_token_id

domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
data_train =[]
for d in domain:
  data = pd.read_csv("./data_{}_preprocessed_train".format(d), sep="\t")
  data_train.append(data)

data_1 = pd.concat([data_train[0], data_train[1]], ignore_index = True)
data_2 = pd.concat([data_train[2], data_train[3]], ignore_index = True)

data_train_src = pd.concat([data_1, data_2], ignore_index = True)
data_train_all =pd.concat([data_train_src, data_train[4]], ignore_index = True)
data_train_trgt = data_train[4]

domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
data_test =[]
for d in domain:
  data = pd.read_csv("./data_{}_preprocessed_test".format(d), sep="\t")
  data_test.append(data)

data_1 = pd.concat([data_test[0], data_test[1]], ignore_index = True)
data_2 = pd.concat([data_test[2], data_test[3]], ignore_index = True)

data_test_src = pd.concat([data_1, data_2], ignore_index = True)
data_test_all =pd.concat([data_test_src, data_test[4]], ignore_index = True)

domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
data_valid =[]
for d in domain:
  data = pd.read_csv("./data_{}_preprocessed_valid".format(d), sep="\t")
  data_valid.append(data)

data_1 = pd.concat([data_valid[0], data_valid[1]], ignore_index = True)
data_2 = pd.concat([data_valid[2], data_valid[3]], ignore_index = True)

data_valid_src = pd.concat([data_1, data_2], ignore_index = True)
data_valid_all =pd.concat([data_valid_src, data_valid[4]], ignore_index = True)
data_valid_trgt = data_valid[4]





data = pd.read_csv("./data_domian_cosine_arabert_all", sep ="\t")
data_train_1 =[]

for i in range(5):
    data_train_1.append(data.sort_values("score_{}".format(domain[i]), ascending = False))
    data_train_1[i] = data_train_1[i][["source_lang_{}".format(domain[i]), "target_lang"]]
    data_train_1[i].rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)




data_all_1 = pd.concat([data_train_1[0][:5000], data_train_1[1][:5000]], ignore_index = True)
data_all_2 = pd.concat([data_train_1[2][:5000], data_train_1[3][:5000]], ignore_index = True)
data_all_3 = pd.concat([data_all_2, data_all_1], ignore_index = True)
data_all_4 = pd.concat([data_all_3, data_train_1[4][:5000]], ignore_index = True)


# data_all_4.drop_duplicates(inplace = True)
# data_all_4.shape
# data_train_all_BT_src  = pd.concat([data_train_src, data_all_3], ignore_index = True)
data_train_all_BT = pd.concat([data_train_src, data_all_3], ignore_index = True)
print(data_all_3.shape)
data_all_3
data_train_src
data_train_all_BT.to_csv("./source_domain_BT", columns = ["target_lang", "source_lang"], index = False )
data_trgt_all_BT = pd.concat([data_train[4], data_all_4], ignore_index = True)

data_1 = pd.concat([data_valid_all, data_train_all], ignore_index = True)
data_all = pd.concat([data_test_all, data_1], ignore_index = True)

data_vocab_all_BT = pd.concat([data_all, data_all_4], ignore_index = True)
# data_vocab_all_BT = pd.concat([data_vocab_all_1, data_all_4], ignore_index = True)
data_vocab_all_BT.shape


MAX_LENGTH = 200
class Lang:
    def __init__(self, name):
        self.name = name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.word2index = { self.tokenizer.pad_token : self.tokenizer.pad_token_id}
        self.word2count = {}
        self.index2word = {self.tokenizer.pad_token_id: self.tokenizer.pad_token}
        self.n_words = 1 # Count PAD token

    def addSentence(self, sentence):
        for word in self.tokenizer.tokenize(sentence, add_special_tokens= True):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.tokenizer.convert_tokens_to_ids(word)
            self.word2count[word] = 1
            self.index2word[self.tokenizer.convert_tokens_to_ids(word)] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

# Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFKC', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters


def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([?.!,¿])", r" \1 ", s)
    s = re.sub(r'[" "]+', " ", s)

    s = re.sub(r"[^a-zA-Z؀-ۿ?.!,¿]+", " ", s)
    s = re.sub(r"([.!?])", r" \1", s)
    # s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

def readLangs(lang1, lang2, reverse=False, label ="train"):
    print("Reading lines...")

    # Read the file and split into lines
    if label=="vocab":
        data = data_vocab_all_BT
    if label =="train":
        data = data_train_all_BT
    if label =="valid":
        data = data_valid_trgt
    if label =="trgt":
        data = data_trgt_all_BT


    # Split every line into pairs and normalize
    pairs = [[normalizeString(data.source_lang[idx]), normalizeString(data.target_lang[idx])] for idx in data.index]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

def prepareData(lang1, lang2, reverse=False, label="train"):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, label)
    print("Read %s sentence pairs" % len(pairs))
    # pairs = pairs[1:]
    # print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        input_lang.addSentence(pair[1])
    output_lang = input_lang
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs



# input_lang, output_lang, pairs = prepareData('Cairo', 'MSA')



def indexesFromSentence(lang, sentence):
    return [lang.word2index.get(word,0) for word in tokenizer.tokenize(sentence, add_special_tokens=True)]

def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)

def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

def get_dataloader(batch_size, label ="train"):
    input_lang, output_lang, _ = prepareData('ar', 'arz', label="vocab")
    _, _, pairs = prepareData('ar', 'arz', label = label)
    # pairs = pairs[1:]
    n = len(pairs)
    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)
    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)

    for idx, (inp, tgt) in enumerate(pairs):
        inp_ids = indexesFromSentence(input_lang, inp)
        tgt_ids = indexesFromSentence(output_lang, tgt)
        inp_ids.append(EOS_token)
        tgt_ids.append(EOS_token)
        input_ids[idx, :len(inp_ids)] = inp_ids
        target_ids[idx, :len(tgt_ids)] = tgt_ids

    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),
                               torch.LongTensor(target_ids).to(device))

    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
    return input_lang, output_lang, train_dataloader

from torch.autograd import Function
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha

        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha

        return output, None

class DomainAdaptationModel(nn.Module):
    def __init__(self,embed_size= 512, hidden_size = 256, dropout=0.1):
        super(DomainAdaptationModel, self).__init__()



        self.dropout = nn.Dropout(dropout)

        self.domain_classifier = nn.Sequential(
            nn.Linear(embed_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 2),
            # nn.LogSoftmax(dim=1),
        )


    def forward(
          self,
         pooled_output,
          grl_lambda = 1.0,
          ):
        pooled_output = self.dropout(pooled_output)


        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)


        domain_pred = self.domain_classifier(reversed_pooled_output)

        return domain_pred.to(device)

class Disc_model(nn.Module):
    def __init__(self, hidden_size=256, input_size=512, dropout =0.1):
        super(Disc_model, self).__init__()

        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(input_size, hidden_size)
        self.tanh = nn.Tanh()
        self.fc = nn.Linear(hidden_size, 2)
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, enc_outputs, label ="trgt"):

        h = self.tanh(self.linear(self.dropout(enc_outputs)))#(N,hid_dim)
        z = self.fc(h) #(N,2)

        # prob = self.logsoftmax(z)


        return z

# input_lang, output_lang, train_dataloader = get_dataloader(32)

# data = iter(train_dataloader)
# next(data)[0]

# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.
class PositionalEncoding(nn.Module):
    def __init__(self,
                 emb_size: int,
                 dropout: float,
                 maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den) # is an indexing operation that targets every other
                                                      #column of the pos_embedding tensor, starting from
                                                      #the second column (index 1) and going up to the last
                                                      #column (index emb_size-1).
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

# helper Module to convert tensor of input indices into corresponding tensor of token embeddings
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

# Seq2Seq Network
class Seq2SeqTransformer(nn.Module):
    def __init__(self,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 emb_size: int,
                 nhead: int,
                 src_vocab_size: int,
                 tgt_vocab_size: int,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1):
        super(Seq2SeqTransformer, self).__init__()
        self.transformer = Transformer(d_model=emb_size,
                                       nhead=nhead,
                                       num_encoder_layers=num_encoder_layers,
                                       num_decoder_layers=num_decoder_layers,
                                       dim_feedforward=dim_feedforward,
                                       dropout=dropout)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)

        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(
            emb_size, dropout=dropout)

    def forward(self,
                src: Tensor,
                trg: Tensor,
                src_mask: Tensor,
                tgt_mask: Tensor,
                src_padding_mask: Tensor,
                tgt_padding_mask: Tensor,
                memory_key_padding_mask: Tensor):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src: Tensor, src_mask: Tensor, src_padding_mask):
        return self.transformer.encoder(self.positional_encoding(
                            self.src_tok_emb(src)), src_mask, src_key_padding_mask = src_padding_mask)

    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor, tgt_padding_mask):
        return self.transformer.decoder(self.positional_encoding(
                          self.tgt_tok_emb(tgt)), memory,
                          tgt_mask, tgt_key_padding_mask = tgt_padding_mask )

def generate_square_subsequent_mask(sz):
    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

PAD_IDX = 0
def create_mask(src, tgt):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]

    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)

    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

PAD_IDX=0
def train_epoch(transformer,src_dataloader,trgt_dataloader,valid_dataloader, optimizer, classifier,domain_optimizer ):
  # if (epoch-1) % int(len(src_dataloader)/ len(trgt_dataloader)) == 0:
  dataset_iter = iter(src_dataloader)
  transformer.train()
  classifier.train()

  total_loss = 0
  total_loss_domain =0
  loss_NMT =0
  trgt_iter = iter(trgt_dataloader)
  for idx in range(len(src_dataloader)):
    input_tensor,target_tensor = next(dataset_iter)

    inp_len = [ len(input_tensor[i][input_tensor[i]!=0]) for i in range(input_tensor.shape[0])]
    trgt_len = [len(target_tensor[i][target_tensor[i]!=0]) for i in range(target_tensor.shape[0]) ]
    input_tensor = input_tensor[:, :max(inp_len)]
    target_tensor = target_tensor[:, :max(trgt_len)]
    input_tensor = input_tensor.T.to(device) #(L,N)
    target_tensor = target_tensor.T.to(device)

    try:
      da_inp,_ = next(trgt_iter)
    except Exception as e:
      trgt_iter = iter(trgt_dataloader)
      da_inp,_ = next(trgt_iter)


    trgt_da_len = [ len(da_inp[i][da_inp[i]!=0]) for i in range(da_inp.shape[0])]
    da_inp_tensor = da_inp[:, :max(trgt_da_len)] #(N,L)
    da_inp_tensor = da_inp_tensor.T.to(device)


    tgt_inp = target_tensor[:-1, :]
    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(input_tensor, tgt_inp)

    encoder_src_domain = transformer.encode(input_tensor, src_mask, src_padding_mask ) #(L,N,embed)
    decoder_src_domain = transformer.decode(tgt_inp, encoder_src_domain, tgt_mask, tgt_padding_mask)
    logits = transformer.generator(decoder_src_domain)

    domain_optimizer.zero_grad()
    src_out_domain = encoder_src_domain.mean(axis=0).to(device) #(N,E)




    y_s_domain = torch.zeros(src_out_domain.shape[0], dtype=torch.long).to(device)


    da_mask = torch.zeros((da_inp_tensor.shape[0], da_inp_tensor.shape[0]),device=DEVICE).type(torch.bool)

    da_padding_mask = (da_inp_tensor == PAD_IDX).transpose(0, 1)
    da_inp_domain = transformer.encode(da_inp_tensor, da_mask, da_padding_mask )

    da_out_domain = da_inp_domain.mean(axis=0).to(device)



    y_t_domain = torch.zeros(da_out_domain.shape[0], dtype=torch.long).to(device)
    y_domain = torch.cat((y_s_domain, y_t_domain))
    domain_out = torch.cat((src_out_domain, da_out_domain))
    domain_pred = classifier(domain_out)

    lprobs = F.log_softmax(domain_pred, dim=-1, dtype=torch.float32)
    lprobs = lprobs.view(-1, lprobs.size(-1))
    loss_domain = F.nll_loss(lprobs, y_domain.view(-1))



    optimizer.zero_grad()
    tgt_out = target_tensor[1:, :]
    # logits = F.log_softmax(logits, dim=-1, dtype=torch.float32)
    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
    loss_NMT +=loss
    loss = loss+ loss_domain

    loss.backward()

    optimizer.step()
    domain_optimizer.step()

    total_loss += loss.item()
    total_loss_domain += loss_domain
  print(total_loss_domain.item()/len(src_dataloader) ,  loss_NMT.item()/len(src_dataloader))
  scheduler.step(total_loss / len((src_dataloader)))
  scheduler_domain.step(total_loss / len(src_dataloader))
  train_end_time = timer()

  transformer.eval()
  valid_losses=0
  valid_start_time = timer()
  for src, tgt in valid_dataloader:
      # Move batch to device
      src = src.to(device)
      tgt = tgt.to(device)

      inp_len = [ len(src[i][src[i]!=0]) for i in range(src.shape[0])]
      trgt_len = [len(tgt[i][tgt[i]!=0]) for i in range(tgt.shape[0]) ]
      input_tensor = src[:, :max(inp_len)]
      target_tensor = tgt[:, :max(trgt_len)]
      src = input_tensor.T.to(device)
      tgt = target_tensor.T.to(device)
      tgt_input = tgt[:-1, :]

      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
      with torch.no_grad():
        logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)



      tgt_out = tgt[1:, :]
      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))

      valid_losses +=loss.item()
  valid_end_time = timer()

  #scheduler.step()


  return total_loss / len(list(src_dataloader)), train_end_time, valid_losses/ len(valid_dataloader), valid_start_time, valid_end_time, loss_NMT.item()/len(src_dataloader)


torch.cuda.empty_cache()
i=0
model ={}
best_loss = - np.inf



batch_size = 16

input_lang, output_lang, src_dataloader = get_dataloader(batch_size= batch_size)
input_lang, output_lang, trgt_dataloader = get_dataloader(batch_size= batch_size, label ="trgt")

input_lang, output_lang, valid_dataloader = get_dataloader(batch_size= batch_size, label ="valid")

# checkpoint = torch.load("./checkpoint_transformer_DC.pt")

# lora_model.load_state_dict(checkpoint['model_state_dict'])
# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# checkpoint = torch.load("./checkpoint_transformer_classifier_DC.pt")

# model.load_state_dict(checkpoint['model_state_dict'])
# domain_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])


torch.cuda.empty_cache()



for z in [0,1,2]:
    torch.manual_seed(z)
    SRC_VOCAB_SIZE = tokenizer.vocab_size
    TGT_VOCAB_SIZE = tokenizer.vocab_size
    EMB_SIZE = 512
    NHEAD = 8
    FFN_HID_DIM = 1024
    
    NUM_ENCODER_LAYERS = 2
    NUM_DECODER_LAYERS = 2
    
    # input_lang, output_lang, src_dataloader = get_dataloader(batch_size= batch_size)
    # input_lang, output_lang, trgt_dataloader = get_dataloader(batch_size= batch_size, label ="trgt")
    
    transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                             NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    transformer = transformer.to(DEVICE)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(transformer.parameters(), lr=0.0001)
    loss_fn_domain_classifier = nn.CrossEntropyLoss()
    # model = DomainAdaptationModel(EMB_SIZE, 256).to(DEVICE) #ADC model
    model = Disc_model(hidden_size = 256, input_size =EMB_SIZE ).to(DEVICE)
    domain_optimizer = optim.Adam(model.parameters(), lr=0.0001)
    scheduler = ReduceLROnPlateau(optimizer, patience=1, verbose = True)
    scheduler_domain = ReduceLROnPlateau(domain_optimizer, patience=1, verbose = True)
    NUM_EPOCHS = 100
    # scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 =NUM_EPOCHS , T_mult=1, verbose = True)
    torch.cuda.empty_cache()
    
    experiments = 3
    
    checkpoint = torch.load("./checkpoint_transformer_UDA_sorce_domain_paper_BT5000_FF1024_AH8_EDL2_{}.pt".format(z))
    
    transformer.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    
    from transformers import AutoModelForSeq2SeqLM
    from peft import LoraModel, LoraConfig
    
    config = LoraConfig(
        task_type="SEQ_2_SEQ_LM",
        r= 64,
        # lora_alpha= 2,
        target_modules=["out_proj", "k_proj","v_proj","q_proj", "embedding"],
        lora_dropout=0.01,
    )
    
    print(transformer)
    lora_model = LoraModel(transformer, config, "default")
    # lora_model = transformer
    optimizer = optim.Adam(lora_model.parameters(), lr=0.0001)
    print(lora_model)
    
    #training with r= 4
    NUM_EPOCHS = 50
    best_valid_loss = 10
    early_stop_threshold = 3
    best_epoch =-1
    j=0
    for epoch in range(1, NUM_EPOCHS+1):
        start_time = timer()
    
        if j < early_stop_threshold:
          train_loss,  train_end_time, valid_loss, valid_start_time, valid_end_time,loss_NMT = train_epoch(lora_model, src_dataloader, trgt_dataloader, valid_dataloader, optimizer, model,domain_optimizer )
          if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            print("Saving ...")
            torch.save({
    
            'model_state_dict': lora_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
    
            }, "./checkpoint_transformer_UDA_DC_BT5000_paper_FF1024_AH8_EDL2_{}.pt".format(z))
            torch.save({
    
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': domain_optimizer.state_dict(),
    
            }, "./checkpoint_classifier_UDA_DC_BT5000_paper_FF1024_AH8_EDL2_{}.pt".format(z))
            j =0
          else:
    
            j +=1
        else:
          break
        print((f"Epoch: {epoch}, Train loss: {train_loss:.9f}, "f"Epoch time = {(train_end_time - start_time)/60:.3f}m"))
        print((f"Epoch: {epoch}, valid loss: {valid_loss:.9f}, "f"Epoch time = {(valid_end_time - valid_start_time)/60:.3f}m"))
    




def greedy_decode(model, src, src_mask,src_padding_mask, max_len, start_symbol, output_lang):
    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)
    decoded_words = []
    attn=[]
    with torch.no_grad():
        memory = model.encode(src, src_mask,src_padding_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        tgt_padding_mask = (ys == PAD_IDX).transpose(0, 1)
        out = model.decode(ys, memory, tgt_mask,tgt_padding_mask )
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()
      
        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        decoded_words.append(output_lang.index2word.get(next_word,output_lang.index2word[1]))
     
        if next_word == EOS_token:
            break
    return decoded_words, ys


# actual function to translate input sentence into target language
def translate(model: torch.nn.Module, data, input_lang, output_lang, n=None):
    targets =[]
    outputs =[]
    model.eval()
    i=0
    for idx in data.index:
      src_sentence = data.source_lang[idx]
      trgt = data.target_lang[idx]
      if i == n:
          break

      src = tensorFromSentence(input_lang, normalizeString(src_sentence)).view(-1, 1)
      num_tokens = src.shape[0]
      src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
      src_padding_mask = (src == PAD_IDX).transpose(0, 1)

      decoded_words, _ = greedy_decode(
          model,  src, src_mask,src_padding_mask,max_len=num_tokens + 5, start_symbol=SOS_token, output_lang=output_lang)
      output_words = tokenizer.convert_tokens_to_string(decoded_words)
      output_words = output_words.replace("[SEP]", "")
      output_words = output_words.replace("[UNK]", "")
      # print("input: ", src_sentence)
      # print("target: ", trgt)
      # print("prediction: ", output_words)
      # print('')
      i=i+1
      targets.append([normalizeString(trgt)])
      outputs.append(output_words)
    return outputs, targets


from nltk.translate.bleu_score import corpus_bleu



for i in [1]:
    bleu={
        "Gulf": [],
            "Iraqi": [],
            "Levantine":[],
            "Nile_Basin":[],
            "North_Africa":[]
          }
    
    # model_path_list = ["./checkpoint_transformer_{}_{}.pt".format(domain[i],x) for x in [0,1,2]]
    model_path_list = ["./checkpoint_transformer_UDA_DC_BT5000_paper_FF1024_AH8_EDL2_{}.pt".format(x)for x in [0,1,2]]
   
    
   
    # checkpoint = torch.load("./checkpoint_transformer_finetune_arabert_large_MADAR_2_{}_BT_4_2_{}.pt".format(domain[i],z))
    z=0
    
    # model_path_list= ["checkpoint_transformer_Gulf.pt"]
    for path in model_path_list:
        
        torch.manual_seed(z)
        z +=1
        transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
        for p in transformer.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        transformer = transformer.to(DEVICE)
        loss_fn = nn.CrossEntropyLoss()
        optimizer = optim.Adam(transformer.parameters(), lr=0.0001)
        checkpoint = torch.load(path)
        config = LoraConfig(
            task_type="SEQ_2_SEQ_LM",
            r= 64,
            # lora_alpha= 2,
            target_modules=["out_proj", "k_proj","v_proj","q_proj", "embedding"],
            lora_dropout=0.01,
        )
        
       
        lora_model = LoraModel(transformer, config, "default")
        print("loading")
        lora_model.load_state_dict(checkpoint['model_state_dict'])
    
    
        for j, (k, v) in enumerate(bleu.items()):
        
            data = data_test[j]
            outputs, target= translate(lora_model, data, input_lang, output_lang)
            src_test = corpus_bleu(target, outputs)
            bleu[k].append(src_test)
            print(bleu)
            src_test



# checkpoint_transformer_UDA_sorce_domain_paper_BT5000_FF1024_AH8_EDL2_, we tested BT with 5000 sentences from src domain (resulting in a total of 20000 (4*5000) sentences)
# {'Gulf': [0.597692287071884,0.6120891477712984, 0.602695063619791 ], 'Iraqi': [0.7327885490200492, 0.7484878639008752, 0.7431371814237584], 'Levantine': [0.5563647864235904, 0.570837285781338, 0.56371365448938], 'Nile_Basin': [0.6024808825091852,0.6171075155887564, 0.6117085135476052], 'North_Africa': [0.31006419113366196, 0.30788594836848027, 0.3140898065682471]}

# checkpoint_transformer_UDA_ADC_BT5000_paper_FF1024_AH8_EDL2_
# {'Gulf': [0.6179000821244964, 0.6277594076919416, 0.6203745939547617], 'Iraqi': [0.7536450402147518, 0.7575000608776833, 0.751932177481225], 'Levantine': [0.5783380131918111, 0.5873790352070682, 0.5809982173640033], 'Nile_Basin': [0.6211198596356012, 0.6325936206515739, 0.6287776543552848], 'North_Africa': [0.3506400277463423, 0.3575387717439608, 0.36318115388804667]}

# checkpoint_transformer_UDA_DC_BT5000_paper_FF1024_AH8_EDL2_
# {'Gulf': [0.6245448191493571, 0.6370679050467101, 0.6299773672472759], 'Iraqi': [0.7608133389823833, 0.76564353521448, 0.7587880578111263], 'Levantine': [0.5808886457566607, 0.5931288345134624, 0.5928020090899364], 'Nile_Basin': [0.6234751408706223, 0.6376761902520207, 0.6379437649030707], 'North_Africa': [0.31743712704446936, 0.32204957675866364, 0.32159541927579255]}



# without BT checkpoint_transformer_UDA_sorce_domain_paper_FF1024_AH8_EDL2_ , source domain, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 2 NUM_DECODER_LAYERS = 2, 
# {'Gulf': [0.5972717001758135, 0.5982771664359354, 0.6034422012067989], 'Iraqi': [0.7709266520650333, 0.7547018598964822, 0.7609667436424117], 'Levantine': [0.5621476509454068, 0.5554040486371232, 0.5661200614711065], 'Nile_Basin': [0.6097261039412042, 0.6066091310921086, 0.6092744869508784], 'North_Africa': [0.3108407161140688, 0.29926032535204045, 0.31036307763415577]}

# checkpoint_transformer_UDA_ADC_paper_FF1024_AH8_EDL2_0,1,2.pt, without BT
# {'Gulf': [0.6150926034607637, 0.6146100254402408, 0.6171038284928421], 'Iraqi': [0.7670844774215064, 0.7714864668957496, 0.7753098153671735], 'Levantine': [0.5776384842268958, 0.5702896583918082, 0.5784287879467972], 'Nile_Basin': [0.6269309914581879, 0.6201960405753802, 0.6203732897790111], 'North_Africa': [0.35572055759965404, 0.357376885528461, 0.35064669692701844]}

# checkpoint_transformer_UDA_DC_paper_FF1024_AH8_EDL2_0,1,2.pt,  without BT
# {'Gulf': [0.6168796181093031, 0.6257558727708805, 0.6220465537099316], 'Iraqi': [0.7786699653530702, 0.7817086806597662, 0.7804221774278557], 'Levantine': [0.5840288549049552, 0.5828722648212041, 0.5835301871294192], 'Nile_Basin': [0.6320310163411039, 0.6326372700635978, 0.6218473314642972], 'North_Africa': [0.3224674459758354, 0.32179543659888427, 0.3205373775325128]}

# checkpoint_transformer_UDA_sorce_domain_paper_BT_FF1024_AH8_EDL2_ , source domain, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 2 NUM_DECODER_LAYERS = 2, 
# {'Gulf': [0.6054117541518728, 0.5938975687446467, 0.5962358160031159], 'Iraqi': [0.7281274929798414, 0.7271356903577093, 0.7263881073961196], 'Levantine': [0.5683099728499966, 0.5577727536269005, 0.5613853073761427], 'Nile_Basin': [0.6076703042472588, 0.6082722191768715, 0.6042972095342416], 'North_Africa': [0.30511795455007135, 0.2963522447641034, 0.2952855732111573]}


# checkpoint_transformer_UDA_ADC_paper_BT_FF1024_AH8_EDL2_0,1,2.pt, ADC with BT

# {'Gulf': [0.6227878009708558, 0.6140197253409473, 0.6119645967089723], 'Iraqi': [0.7429263294349058, 0.7382857618120273, 0.7406300557799952], 'Levantine': [0.584786713056264, 0.5808709815864234, 0.5760627580193812], 'Nile_Basin': [0.6298423435336828, 0.623359741849474, 0.620045310753457], 'North_Africa': [0.3436668826798621, 0.3497508283760583, 0.34028914413755784]}


# checkpoint_transformer_UDA_DC_paper_BT_FF1024_AH8_EDL2_,1,2.pt, DC with BT

# {'Gulf': [0.626383905132067, 0.6170830210123962, 0.6126902896094152], 'Iraqi': [0.7368539759087351, 0.7325217695782366, 0.7434154249494448], 'Levantine': [0.5896413299805755, 0.5824222254630043, 0.5796401388353694], 'Nile_Basin': [0.6319890749914165, 0.6228441387763844, 0.622964738684482], 'North_Africa': [0.30366144765704667, 0.3092083361968178, 0.30889674808593187]}



# base_model with BT for source domain, all, checkpoint_transformer_UDA_base_BT_0,1,2.pt
# {'Gulf': [0.5894374444476336, 0.5947829009169366, 0.5777648771893217], 'Iraqi': [0.7052802494627243, 0.7088465623828415, 0.702550310463655], 'Levantine': [0.5505838938451393, 0.5628277527659523, 0.5479533445537093], 'Nile_Basin': [0.5907274419230469, 0.6042037863807745, 0.5898945106936381], 'North_Africa': [0.2967718227233345, 0.29833520326917046, 0.29389457539225117]}

# ADC, checkpoint_transformer_UDA_ADC_BT_ 
# {'Gulf': [0.6115091918414131, 0.6134813515567112, 0.6009012303791632], 'Iraqi': [0.7367862677981785, 0.7385018135496851, 0.7229568405894765], 'Levantine': [0.5734499761669609, 0.5808469587174571, 0.5679292654640727], 'Nile_Basin': [0.6136147975723871, 0.617567006125695, 0.6087002797092476], 'North_Africa': [0.33395512869180627, 0.33942147341798085, 0.334265808551596]}


# DC, checkpoint_transformer_UDA_DC_BT_ 

# {'Gulf': [0.6135455353348839, 0.6172338057474719, 0.6020743735735401], 'Iraqi': [0.7257391880517167, 0.7306777512801393, 0.7270917015030153], 'Levantine': [0.5757668121784619, 0.584969278219306, 0.5679583084900679], 'Nile_Basin': [0.617932599626661, 0.6188289705446364, 0.6094996162927435], 'North_Africa': [0.3021414443140225, 0.30716459381475547, 0.30272694650423504]}


# base_model without BT for source domain, all, checkpoint_transformer_UDA_base_0,1,2.pt
# {'Gulf': [0.5939609674406768, 0.5870850926482667, 0.580940017044124], 'Iraqi': [0.7534491376722473, 0.7342764336179748, 0.7300846230446586], 'Levantine': [0.5618099367887162, 0.5486210656431958, 0.5460161894118924], 'Nile_Basin': [0.6027746380586632, 0.5974342373306974, 0.5823087977374283], 'North_Africa': [0.3027583772678766, 0.30269599168004524, 0.30039768026402225]}

# DC model without BT
# {'Gulf': [0.6133236498235609, 0.6083684382337808, 0.5980890057962781], 'Iraqi': [0.7678897804489329, 0.7657431904893828, 0.7507607820709292], 'Levantine': [0.5777159422940611, 0.5769090599653397, 0.5656167922027793], 'Nile_Basin': [0.6214761510086763, 0.6200804429320039, 0.6088437865222146], 'North_Africa': [0.3183237966654917, 0.3121721970965041, 0.3128607335572652]}
# ADC model without BT
# {'Gulf': [0.6073956605725336, 0.6045796243933722, 0.5964697276863009], 'Iraqi': [0.7571483683556677, 0.7501779202386475, 0.735482617615984], 'Levantine': [0.5736589104019152, 0.5687384256533101, 0.5616946728268221], 'Nile_Basin': [0.612205836217077, 0.6121683884043712, 0.6035585080460019], 'North_Africa': [0.34828101622750335, 0.3456608748161917, 0.3382858160207485]}













# model_path_list = ["checkpoint_transformer_{}.pt".format(d) for d in domain ]
# domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]


# # data_test_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
# bleu={"Gulf": [],
#       "Iraqi": [],
#       "Levantine":[],
#       "Nile_Basin":[],
#       "North_Africa":[]}

# # data_length={"Gulf": [],
# #       "Iraqi": [],
# #       "Levantine":[],
# #       "Nile_Basin":[],
# #       "North_Africa":[]}

# for model_path in model_path_list:

#     checkpoint = torch.load(model_path)
#     transformer.load_state_dict(checkpoint['model_state_dict'])

#     for i, (k, v) in enumerate(bleu.items()):
#         data = data_test[i]
#         outputs, target= translate(transformer, data, input_lang, output_lang)
#         # data_length[k].append((outputs, target))
#         src_test = corpus_bleu(target, outputs)
#         bleu[k].append(src_test)
#         print(bleu)
 
#Transformer models per dialect MT model ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ], this is the order
# {'Gulf': [0.437167863084716, 0.2897383917189309, 0.35415628144967637, 0.31344305513686493, 0.3456923907384509], 
# 'Iraqi': [0.38541595370298104, 0.47066826294896486, 0.4210838360425771, 0.3124180248515549, 0.35077855648506373], 
# 'Levantine': [0.25161119831076456, 0.2222340874702187, 0.4529012495991333, 0.2566760059519127, 0.2908829814246059], 
# 'Nile_Basin': [0.3201301742681078, 0.22852806751590213, 0.35464937348770276, 0.4298559601118441, 0.32629691780245457],
# 'North_Africa': [0.2005469782712749, 0.16945247398433594, 0.23158390228178818, 0.19713916586179814, 0.41319173879502985]}
      
  
      
#              "Gulf"   "Iraqi"   "Levantine"  "Nile_Basin"   "North_Africa"      
# Gulf model   43.7      38.54     25.1           32.0          20.0


    
#can create output and target directly from saved file for ADC and base model
# for k in domain:
#     data = pd.read_csv("./translated_transformer_base_{}".format(k))
    
#     print(data.shape,k)
#     targets = [ [data["target_lang"][idx]] for idx in data.index]
#     outputs = data["trgt_prediction"]
#     data_length[k].append((outputs, targets))
#     print(len(data_length[k]))       
        
        
# print(data_length["Gulf"])

# for l in data_length["Gulf"]:
#     x,y = l
#     print(x[0], y[0])
# length_dict= {"Gulf": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Iraqi": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Levantine": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Nile_Basin": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "North_Africa": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} }
# for idx, (k,v) in enumerate(data_length.items()):
#     outputs, targets = data_length[k][0]
#     print(len(targets),k)
#     for i in range(len(targets)):
        
#         sent_len = len(tokenizer.tokenize(data_test[idx].source_lang[i]))
#         if sent_len <= 5:
#             length_dict[k]["<5"] += [(outputs[i],targets[i] )]
#         if 5<sent_len <= 10:
#             length_dict[k]["(5-10)"] += [(outputs[i],targets[i] )]
#         if 10<sent_len <= 15:
#             length_dict[k]["(10-15)"] += [(outputs[i],targets[i] )]
#         if 15<sent_len <= 20:
#             length_dict[k]["(15-20)"] += [(outputs[i],targets[i] )]
#         if 20 <sent_len <= 25:
#             length_dict[k]["(20-25)"] += [(outputs[i],targets[i] )]
#         if sent_len > 25:
#             length_dict[k][">25"] += [(outputs[i],targets[i] )]
# bleu_dict= {"Gulf": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Iraqi": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Levantine": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Nile_Basin": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "North_Africa": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} }     


# length_distribution= {"Gulf": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Iraqi": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Levantine": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Nile_Basin": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "North_Africa": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} }     

# for k,v in length_dict.items():
    
#     for k_2, v_2 in v.items():
#         outs = []
#         trgts =[]
#         for outputs, targets in length_dict[k][k_2]:
#             outs.append(outputs)
#             trgts.append(targets)
#         print(len(outs), len(trgts))
#         length_distribution[k][k_2].append(len(outs))
#         src_test = corpus_bleu(trgts, outs)
#         bleu_dict[k][k_2].append(src_test)
# bleu_dict
# length_distribution


# bleu_domains= {"source": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Target": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []}}

# length_domains= {"source": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Target": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []}}


# for k,v in bleu_dict.items():
    
#     for k_2, v_2 in v.items():
        
#         if k != "North_Africa":
            
#             bleu_domains["source"][k_2] += v_2
#         else:
#             bleu_domains["Target"][k_2] += v_2

# for k_2, v_2 in bleu_domains["source"].items():
#     bleu_domains["source"][k_2] = np.array(bleu_domains["source"][k_2]).mean()


# for k,v in length_distribution.items():
    
#     for k_2, v_2 in v.items():
        
#         if k != "North_Africa":
            
#             length_domains["source"][k_2] += v_2
#         else:
#             length_domains["Target"][k_2] += v_2

# for k_2, v_2 in length_domains["source"].items():
#     length_domains["source"][k_2] = np.array(length_domains["source"][k_2]).sum()


# length_domains
# bleu_domains 
# x,y = data_length["Gulf"][0]
# print(len(y))
# for i, (k, v) in enumerate(data_length.items()):
    
  
#     output, target = data_length[k][0]

#     data_save = pd.DataFrame(data = {"trgt_prediction": output, "target_lang" : [target[idx][0] for idx in range(len(target))], "source_lang":data_test[i].source_lang })
#     data_save.to_csv("./translated_transformer_DC_{}".format(k))
# data_save.target_lang[0]
# length_distribution





# x, y = data_length["Gulf"][0]
# print(len(y))

#Transformer DC

# bleu_dict
# {'Gulf': {'<5': [0.5767386438641703],
#   '(5-10)': [0.604382341358195],
#   '(10-15)': [0.6037734137515678],
#   '(15-20)': [0.6321742652567801],
#   '(20-25)': [0.6063457602794765],
#   '>25': [0.6845419662031338]},
#  'Iraqi': {'<5': [0.7059465584595608],
#   '(5-10)': [0.7487315880055264],
#   '(10-15)': [0.7604321694967233],
#   '(15-20)': [0.7982154437663056],
#   '(20-25)': [0.8221258147523858],
#   '>25': [0.8449711384504857]},
#  'Levantine': {'<5': [0.5390450960741754],
#   '(5-10)': [0.6044755859101689],
#   '(10-15)': [0.5849613975132962],
#   '(15-20)': [0.5097423716255002],
#   '(20-25)': [0.47165794512337833],
#   '>25': [0.514714938831084]},
#  'Nile_Basin': {'<5': [0.5680351304992012],
#   '(5-10)': [0.6133199177696462],
#   '(10-15)': [0.6113525057534637],
#   '(15-20)': [0.6228693058948485],
#   '(20-25)': [0.6469975490783371],
#   '>25': [0.6216719477064191]},
#  'North_Africa': {'<5': [0.2623215290181021],
#   '(5-10)': [0.29530841013970494],
#   '(10-15)': [0.314720008444911],
#   '(15-20)': [0.31534798946505144],
#   '(20-25)': [0.31293257390620716],
#   '>25': [0.3846934191247483]}}

#Bleu domains
# {'source': {'<5': 0.5974413572242769,
#   '(5-10)': 0.6427273582608841,
#   '(10-15)': 0.6401298716287628,
#   '(15-20)': 0.6407503466358586,
#   '(20-25)': 0.6367817673083944,
#   '>25': 0.6664749977977806},
#  'Target': {'<5': [0.2623215290181021],
#   '(5-10)': [0.29530841013970494],
#   '(10-15)': [0.314720008444911],
#   '(15-20)': [0.31534798946505144],
#   '(20-25)': [0.31293257390620716],
#   '>25': [0.3846934191247483]}}








#Transformer ADC

# bleu_dict
# {'Gulf': {'<5': [0.56503260552475],
#   '(5-10)': [0.5962937930606398],
#   '(10-15)': [0.5939556497183516],
#   '(15-20)': [0.6063055141555751],
#   '(20-25)': [0.5801549836155185],
#   '>25': [0.6615172351458252]},
#  'Iraqi': {'<5': [0.6976758657681977],
#   '(5-10)': [0.7471076030745133],
#   '(10-15)': [0.771587771528094],
#   '(15-20)': [0.7975860094523345],
#   '(20-25)': [0.8130452117626533],
#   '>25': [0.9138382712459046]},
#  'Levantine': {'<5': [0.5222391635229411],
#   '(5-10)': [0.5945726888193765],
#   '(10-15)': [0.578811628785203],
#   '(15-20)': [0.5089399550150048],
#   '(20-25)': [0.45932582261964994],
#   '>25': [0.5167408126124655]},
#  'Nile_Basin': {'<5': [0.5533736858500824],
#   '(5-10)': [0.6066332643186816],
#   '(10-15)': [0.6030174607099101],
#   '(15-20)': [0.6208792899794829],
#   '(20-25)': [0.6454354958542387],
#   '>25': [0.59509185371254]},
#  'North_Africa': {'<5': [0.28659224088245666],
#   '(5-10)': [0.337662630958748],
#   '(10-15)': [0.34982600576897366],
#   '(15-20)': [0.3340508417025362],
#   '(20-25)': [0.3184320891056185],
#   '>25': [0.391036276386829]}}

#Bleu domains
# {'source': {'<5': 0.5845803301664928,
#   '(5-10)': 0.6361518373183028,
#   '(10-15)': 0.6368431276853896,
#   '(15-20)': 0.6334276921505994,
#   '(20-25)': 0.6244903784630151,
#   '>25': 0.6717970431791838},
#  'Target': {'<5': [0.28659224088245666],
#   '(5-10)': [0.337662630958748],
#   '(10-15)': [0.34982600576897366],
#   '(15-20)': [0.3340508417025362],
#   '(20-25)': [0.3184320891056185],
#   '>25': [0.391036276386829]}}


# length
# {'Gulf': {'<5': [668],
#   '(5-10)': [1824],
#   '(10-15)': [510],
#   '(15-20)': [152],
#   '(20-25)': [46],
#   '>25': [21]},
#  'Iraqi': {'<5': [141],
#   '(5-10)': [543],
#   '(10-15)': [199],
#   '(15-20)': [60],
#   '(20-25)': [16],
#   '>25': [15]},
#  'Levantine': {'<5': [1071],
#   '(5-10)': [2736],
#   '(10-15)': [1184],
#   '(15-20)': [429],
#   '(20-25)': [123],
#   '>25': [126]},
#  'Nile_Basin': {'<5': [538],
#   '(5-10)': [1757],
#   '(10-15)': [726],
#   '(15-20)': [198],
#   '(20-25)': [48],
#   '>25': [50]},
#  'North_Africa': {'<5': [1522],
#   '(5-10)': [4670],
#   '(10-15)': [2255],
#   '(15-20)': [783],
#   '(20-25)': [277],
#   '>25': [242]}}

# {'source': {'<5': 2418,
#   '(5-10)': 6860,
#   '(10-15)': 2619,
#   '(15-20)': 839,
#   '(20-25)': 233,
#   '>25': 212},
#  'Target': {'<5': [1522],
#   '(5-10)': [4670],
#   '(10-15)': [2255],
#   '(15-20)': [783],
#   '(20-25)': [277],
#   '>25': [242]}}


#Transformer BASE
# {'source': {'<5': 0.5774311792676974,
#   '(5-10)': 0.6255561699323323,
#   '(10-15)': 0.618970837589611,
#   '(15-20)': 0.6117252276380103,
#   '(20-25)': 0.5943712645455415,
#   '>25': 0.6532720369503162},
#  'Target': {'<5': [0.26184336330603075],
#   '(5-10)': [0.28166798574704566],
#   '(10-15)': [0.303013638223251],
#   '(15-20)': [0.32261911776473357],
#   '(20-25)': [0.29861232740529553],
#   '>25': [0.3833965113373046]}}

#bleu per dialect per length
#{'Gulf': {'<5': [0.5601967464891785],
 #  '(5-10)': [0.589731117772431],
 #  '(10-15)': [0.5876739969753156],
 #  '(15-20)': [0.6185208161845491],
 #  '(20-25)': [0.5930119209836825],
 #  '>25': [0.6499818375439446]},
 # 'Iraqi': {'<5': [0.6882032425216033],
 #  '(5-10)': [0.7367901372302574],
 #  '(10-15)': [0.7340928587929371],
 #  '(15-20)': [0.73760223212171],
 #  '(20-25)': [0.74359417319891],
 #  '>25': [0.820611154156819]},
 # 'Levantine': {'<5': [0.5120200916279407],
 #  '(5-10)': [0.5822076379473625],
 #  '(10-15)': [0.5646099649979218],
 #  '(15-20)': [0.49336906114446555],
 #  '(20-25)': [0.4310393350213555],
 #  '>25': [0.4941735902891755]},
 # 'Nile_Basin': {'<5': [0.5493046364320673],
 #  '(5-10)': [0.5934957867792785],
 #  '(10-15)': [0.5895065295922699],
 #  '(15-20)': [0.5974088011013168],
 #  '(20-25)': [0.6098396289782179],
 #  '>25': [0.6483215658113256]},
 # 'North_Africa': {'<5': [0.26184336330603075],
 #  '(5-10)': [0.28166798574704566],
 #  '(10-15)': [0.303013638223251],
 #  '(15-20)': [0.32261911776473357],
 #  '(20-25)': [0.29861232740529553],
 #  '>25': [0.3833965113373046]}}










# from nltk.translate.bleu_score import corpus_bleu

# model_path_list = ["checkpoint_transformer_DC.pt", "checkpoint_transformer_ADC.pt"]
# domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]


# # data_test_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
# bleu={"Gulf": [],
#       "Iraqi": [],
#       "Levantine":[],
#       "Nile_Basin":[],
#       "North_Africa":[]}


# for model_path in model_path_list:

#   checkpoint = torch.load(model_path)
#   lora_model.load_state_dict(checkpoint['model_state_dict'])
#   for _ in range(2):
#       for i, (k, v) in enumerate(bleu.items()):
#           data = data_test[i]
#           outputs, target= translate(lora_model, data, input_lang, output_lang)
#           src_test = corpus_bleu(target, outputs)
#           bleu[k].append(src_test)
#           print(bleu)
# print(bleu)


# [Transformer DC,DC, ADC,ADC]
# {'Gulf': [0.606335978026533, 0.606335978026533, 0.5947232471198883, 0.5947232471198883],
 # 'Iraqi': [0.7628058151748683, 0.7628058151748683, 0.7676443610428514, 0.7676443610428514],
 # 'Levantine': [0.5701613547597201, 0.5701613547597201, 0.5630182180027015, 0.5630182180027015],
 # 'Nile_Basin': [0.6117142648772141, 0.6117142648772141, 0.6039170252813371, 0.6039170252813371], 
 # 'North_Africa': [0.31174888307182846, 0.31174888307182846, 0.34639018513774855, 0.34639018513774855]}



#{'North_Africa': [0.34639018513774855]}, r=64 , ["out_proj", "k_proj","v_proj","q_proj", "embedding"], ADC
# {'North_Africa': [0.31174888307182846]}, , r=64 , ["out_proj", "k_proj","v_proj","q_proj", "embedding"], DC

# 0.2994663491121652 src train







# {'North_Africa': [0.34674611792068766]}, r=64 , choice in target_modules = "out_proj", "embedding", ADC
# {'North_Africa': [0.31137739453972996]}, r=64 , choice in target_modules = "out_proj", "embedding", DC













# bleu  {'North_Africa': [0.30870897007317993]} DC result, r= 256, target_modules = "out_proj", "embedding"]
# {'North_Africa': [0.3017217028730928]}, r=64
# {'North_Africa': [0.31]}, r=64 make labels both equal to zeros, target_modules = "out_proj", "embedding"]


# bleu 0.3295556070841388 ADC r=4
# bleu {'North_Africa': [0.3356997120186674]} ,r =8
# {'North_Africa': [0.3401193678432203]}, r=16
#{'North_Africa': [0.3432325014977175]}, r=32
# {'North_Africa': [0.34970803345426127]}, r=64 , choice in target_modules = "linear1", "linear2", "embedding"




# {'North_Africa': [0.35017518729245356]} , r= 256 , target_modules = "out_proj", "embedding"], ADC model

# from nltk.translate.bleu_score import corpus_bleu

# model_path_list = ["checkpoint_transformer_ADC.pt"]
# domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]


# # data_test_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
# bleu={
#    "North_Africa":[]}

# for model_path in model_path_list:

#   checkpoint = torch.load(model_path)
#   lora_model.load_state_dict(checkpoint['model_state_dict'])
#   for i, (k, v) in enumerate(bleu.items()):
#       data = data_test[4]
#       outputs, target= translate(lora_model, data, input_lang, output_lang)
#       src_test = corpus_bleu(target, outputs)
#       bleu[k].append(src_test)
# bleu

# bleu

# # from nltk.translate.bleu_score import corpus_bleu

# # # bleu={"Iraqi": 0,
# # #       "Levantine": 0,
# # #       "GULF":0,
# # #       "Nile_Basin":0,
# # #       "North_Africa":0}
# # # data = [data_test_Iraqi, data_test_Levantine, data_test_GULF, data_test_Nile_Basin, data_test_North_Africa]


# # checkpoint = torch.load("./checkpoint-transformer-GULF-1024layers.pt")

# # transformer.load_state_dict(checkpoint['model_state_dict'])
# # bleu = {
# #     "src":0,
# #     "trgt":0
# # }
# # data = [data_test_src, data_test_trgt]
# # for i, (k, v) in enumerate(bleu.items()):

#     data_test = data[i]
#     outputs, target= translate(transformer, data_test, input_lang, output_lang)
#     src_test = corpus_bleu(target, outputs)
#     bleu[k] = src_test




# # outputs, target= translate(transformer, data_test_Iraqi, input_lang, output_lang)
# # src_test_Iraqi = corpus_bleu(target, outputs)
# # bleu["Iraqi"] = src_test_Iraqi
# # print(src_test)
# # average_bleu = np.array(bleu).mean(axis=0)
# # std_bleu = np.array(bleu).std(axis=0)

# print(["Src test average Bleu score for {} dialect is {}".format(k,v) for k,v in bleu.items()])
# # EMB_SIZE = [128,256,512]
# # NHEAD = [2,4,8]
# # bleu={"src":[i for i in range(9)], "trgt":[i for i in range(9)]}
# # print(bleu["src"] )
# # i=6

# # embed_head = [ (512,2), (512,4)]
# # for embed, nhead in embed_head:
# #     print(embed, nhead)
# #     num_layers = nhead

# #     transformer = Seq2SeqTransformer(num_layers, num_layers, embed,
# #                               nhead, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, embed)
# #     for p in transformer.parameters():
# #         if p.dim() > 1:
# #             nn.init.xavier_uniform_(p)
# #     transformer = transformer.to(DEVICE)
# #     i= i+1
# #     # if i==6:
# #     #     continue
# #     checkpoint= torch.load(f"./checkpoint-transformer-MAG{i}.pt")
# #     transformer.load_state_dict(checkpoint["model_state_dict"])
# #     # optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
# #     outputs, target= translate(transformer, trgt_test, input_lang, output_lang)
# #     trgt_bleu = corpus_bleu(target, outputs)


# #     bleu["trgt"][i-1] = trgt_bleu



# # model[i]= transformer
# # i=0
# # outputs, target= translate(transformer, trgt_test, input_lang, output_lang)

# # trgt_bleu = corpus_bleu(target, outputs)
# # bleu["trgt"][i-1] = trgt_bleu

# # outputs, target= translate(transformer, data_test, input_lang, output_lang)

# # src_test = corpus_bleu(target, outputs)
# # bleu["src"][i-1] = src_test

# # print( bleu)

# # print(data_test.shape, trgt_test.shape)

# # model_type= [f"embed+hid: {embed} , head+layers: {nhead}" for embed in EMB_SIZE  for nhead in NHEAD]
# # print( model_type)
# # data=pd.DataFrame(columns = ["model_type", "bleu_src", "bleu_trgt"])
# # # data["model"] = model
# # data["bleu_src"] = [ i*100 for i in bleu["src"]]
# # data["bleu_trgt"] = [ i*100 for i in bleu["trgt"]]
# # data["model_type"] = model_type
# # print(data)
# # data.to_csv("./model_type_transformers.csv", sep=",")
# # def evaluate(model):
# #     model.eval()
# #     losses = 0



# #     for src, tgt in val_dataloader:
# #         src = src.to(DEVICE)
# #         tgt = tgt.to(DEVICE)

# #         tgt_input = tgt[:-1, :]

# #         src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

# #         logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)

# #         tgt_out = tgt[1:, :]
# #         loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
# #         losses += loss.item()

# #     return losses / len(list(val_dataloader))