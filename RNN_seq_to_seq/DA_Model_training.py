# -*- coding: utf-8 -*-
"""pytorch_seq_seq_with_Banhandu_attention_batch_size_arabert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zYUFUCY4u0wFK_M0WwI33N2mhkpsRxEl
"""

from __future__ import unicode_literals, print_function, division

from arabert.preprocess import ArabertPreprocessor
import torch
import torch.nn as nn
from torch import optim

from transformers import AutoTokenizer

import pandas as pd

from utils import *
from Decoder_Encoder import *
from Domain_adaptation import *
model_name = "aubmindlab/bert-base-arabertv02-twitter"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_name)




data_train_src= pd.read_csv("././Data/data_src_train_MAG", sep="\t")

data_test_src = pd.read_csv("./Data/data_test_src_MAG" , sep="\t")
data_trgt_train = pd.read_csv("./Data/data_trgt_train_MAG" , sep="\t")
data_trgt_test = pd.read_csv("./Data/data_test_trgt_MAG" , sep="\t")


SOS_token = tokenizer.cls_token_id
EOS_token = tokenizer.sep_token_id
PAD_token = tokenizer.pad_token_id

arabic_prep = ArabertPreprocessor(model_name)


def train(src_dataloader,trgt_dataloader,  encoder, decoder,model, n_epochs, learning_rate=0.001,
               print_every=100):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every


    criterion = nn.NLLLoss()
    loss_fn_domain_classifier = nn.NLLLoss()
    

    for epoch in range(1, n_epochs + 1):
        if (epoch-1) % int(len(src_dataloader)/ len(trgt_dataloader)) == 0:
            dataset_iter = iter(src_dataloader)
            
            
        total_loss = 0
        trgt_iter = iter(trgt_dataloader)
        for idx in range(int(len(trgt_dataloader))):

            input_tensor,target_tensor = next(dataset_iter)
            da_inp,_ = next(trgt_iter)
            inp_len = [ len(input_tensor[i][input_tensor[i]!=0]) for i in range(input_tensor.shape[0])]
            trgt_len = [len(target_tensor[i][target_tensor[i]!=0]) for i in range(target_tensor.shape[0]) ]
            
            trgt_nmt_len = [ len(da_inp[i][da_inp[i]!=0]) for i in range(da_inp.shape[0])]
            da_inp_tensor = da_inp[:, :max(trgt_nmt_len)]
            
            input_tensor = input_tensor[:, :max(inp_len)]
            target_tensor = target_tensor[:, :max(trgt_len)]
            


            encoder_optimizer.zero_grad()
            decoder_optimizer.zero_grad()
            domain_optimizer.zero_grad()

            encoder_outputs, encoder_hidden = encoder(input_tensor)
            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor, MAX_LENGTH= max(trgt_len))
            context, _ = attn(encoder_hidden.permute(1,0,2),encoder_outputs )
            
            loss = criterion(
                decoder_outputs.view(-1, decoder_outputs.size(-1)),
                target_tensor.reshape(-1)
            )
            
            encoder_outputs_trgt , encoder_hidden_trgt = encoder(da_inp_tensor)
            context_trgt, _ = attn(encoder_hidden_trgt.permute(1,0,2),encoder_outputs_trgt )
            
            domain_pred = model(context.squeeze(1))
            
            y_s_domain = torch.zeros(input_tensor.shape[0], dtype=torch.long).to(device)
            loss_s_domain = loss_fn_domain_classifier(domain_pred, y_s_domain)

            domain_pred_trgt = model(context_trgt.squeeze(1))
            
            y_t_domain = torch.zeros(da_inp.shape[0], dtype=torch.long).to(device)
            loss_t_domain = loss_fn_domain_classifier(domain_pred_trgt, y_t_domain)

      
            loss = loss + loss_t_domain + loss_s_domain
            
            loss.backward()
        
            encoder_optimizer.step()
            decoder_optimizer.step()
            domain_optimizer.step()

            total_loss += loss.item()
        scheduler_enc.step(total_loss / len(trgt_dataloader))
        scheduler_dec.step(total_loss / len(trgt_dataloader))
        
        
        print_loss_total += loss
        plot_loss_total += loss

        if epoch % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),
                                        epoch, epoch / n_epochs * 100, print_loss_avg))

        if epoch % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0





hidden_size = 512
batch_size = 8
learning_rate = 0.0001
from torch.optim.lr_scheduler import ReduceLROnPlateau




input_lang, output_lang, src_dataloader = get_dataloader(batch_size)
input_lang, output_lang, trgt_dataloader = get_dataloader(batch_size, label ="trgt")


encoder = EncoderRNN(tokenizer.vocab_size, hidden_size).to(device)
decoder = AttnDecoderRNN(hidden_size, tokenizer.vocab_size).to(device)

#Domain adaptation model can be defined by backpropogation or a feedforward network discriminator
model = DomainAdaptationModel(embed_size = hidden_size).to(device)
attn = BahdanauAttention(hidden_size).to(device)
# model = Disc_model(input_size= hidden_size)

encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)
domain_optimizer = optim.Adam(model.parameters(), lr= learning_rate)

scheduler_enc = ReduceLROnPlateau(encoder_optimizer, patience=2, factor = 0.1, verbose = True)
scheduler_dec = ReduceLROnPlateau(decoder_optimizer, patience=2, factor = 0.1, verbose = True)
torch.cuda.empty_cache()
Load= True
save = False



if Load:
    load_checkpoit("./Models/checkpoint-enc-arabert-da-MAG-context.pt", encoder, encoder_optimizer)
    load_checkpoit("./Models/checkpoint-dec-arabert-da-MAG-context.pt", decoder, decoder_optimizer)
    load_checkpoit("./Models/checkpoint-model-arabert-da-MAG-context.pt", model, domain_optimizer)
    

    
train(src_dataloader,trgt_dataloader,  encoder, decoder,model, n_epochs=60, print_every=1) #model trained already on 60 epochs. 

if save:
    save_checkpoit("./Models/checkpoint-enc-arabert-da-MAG-context.pt", encoder, encoder_optimizer)
    save_checkpoit("./Models/checkpoint-dec-arabert-da-MAG-context.pt", decoder, decoder_optimizer)
    load_checkpoit("./Models/checkpoint-model-arabert-da-MAG-context.pt", decoder, decoder_optimizer)


from nltk.translate.bleu_score import corpus_bleu
targets, outputs= evaluateRandomly(encoder, decoder,input_lang,output_lang, data_test_src)
src_test = corpus_bleu(targets, outputs )
targets_trgt, outputs_trgt= evaluateRandomly(encoder, decoder,input_lang,output_lang, data_trgt_test)
trgt_test = corpus_bleu(targets_trgt, outputs_trgt )

print("Bley score for target domain is {} and for source domain is {}".format(trgt_test,src_test ))
