# -*- coding: utf-8 -*-
"""pytorch_seq_seq_with_Banhandu_attention_batch_size_arabert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zYUFUCY4u0wFK_M0WwI33N2mhkpsRxEl
"""

from __future__ import unicode_literals, print_function, division

from arabert.preprocess import ArabertPreprocessor
import torch
import torch.nn as nn
from torch import optim

from transformers import AutoTokenizer

import pandas as pd

from utils import *
from Decoder_Encoder import *
model_name = "aubmindlab/bert-base-arabertv02-twitter"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_name)




data_train_src= pd.read_csv("././Data/data_src_train_MAG", sep="\t")

data_test_src = pd.read_csv("./Data/data_test_src_MAG" , sep="\t")
data_trgt_train = pd.read_csv("./Data/data_trgt_train_MAG" , sep="\t")
data_trgt_test = pd.read_csv("./Data/data_test_trgt_MAG" , sep="\t")


SOS_token = tokenizer.cls_token_id
EOS_token = tokenizer.sep_token_id
PAD_token = tokenizer.pad_token_id

arabic_prep = ArabertPreprocessor(model_name)


def train(train_dataloader, encoder, decoder, n_epochs,
               print_every=100, plot_every=100):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every


    criterion = nn.NLLLoss()

    for epoch in range(1, n_epochs + 1):
        total_loss = 0
        for data in train_dataloader:
            input_tensor, target_tensor = data
            inp_len = [ len(input_tensor[i][input_tensor[i]!=0]) for i in range(input_tensor.shape[0])]
            trgt_len = [len(target_tensor[i][target_tensor[i]!=0]) for i in range(target_tensor.shape[0]) ]
            input_tensor = input_tensor[:, :max(inp_len)]
            target_tensor = target_tensor[:, :max(trgt_len)]


            encoder_optimizer.zero_grad()
            decoder_optimizer.zero_grad()

            encoder_outputs, encoder_hidden = encoder(input_tensor)
            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor, MAX_LENGTH= max(trgt_len))

            loss = criterion(
                decoder_outputs.view(-1, decoder_outputs.size(-1)),
                target_tensor.reshape(-1)
            )
          
            loss.backward()

            encoder_optimizer.step()
            decoder_optimizer.step()

            total_loss += loss.item()
           
        scheduler_enc.step(total_loss / len(train_dataloader))
        scheduler_dec.step(total_loss / len(train_dataloader))
        print_loss_total += loss
        plot_loss_total += loss

        if epoch % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),
                                        epoch, epoch / n_epochs * 100, print_loss_avg))

        if epoch % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0





hidden_size = 512
batch_size = 8
learning_rate = 0.0001
from torch.optim.lr_scheduler import ReduceLROnPlateau




input_lang, output_lang, train_dataloader = get_dataloader(batch_size)


encoder = EncoderRNN(tokenizer.vocab_size, hidden_size).to(device)
decoder = AttnDecoderRNN(hidden_size, tokenizer.vocab_size).to(device)

encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)
scheduler_enc = ReduceLROnPlateau(encoder_optimizer, patience=2, factor = 0.1, verbose = True)
scheduler_dec = ReduceLROnPlateau(decoder_optimizer, patience=2, factor = 0.1, verbose = True)
torch.cuda.empty_cache()
Load= True
save = False



if Load:
    load_checkpoit("./Models/checkpoint-enc-arabert-MAG.pt", encoder, encoder_optimizer)
    load_checkpoit("./Models/checkpoint-dec-arabert-MAG.pt", decoder, decoder_optimizer)
    

    
train(train_dataloader, encoder, decoder, 60, print_every=1, plot_every=5) #model trained already on 60 epochs. 

if save:
    save_checkpoit("./Models/checkpoint-enc-arabert-MAG", encoder, encoder_optimizer)
    save_checkpoit("./Models/checkpoint-dec-arabert-MAG", decoder, decoder_optimizer)


from nltk.translate.bleu_score import corpus_bleu
targets, outputs= evaluateRandomly(encoder, decoder,input_lang,output_lang, data_test_src)
src_test = corpus_bleu(targets, outputs )
targets_trgt, outputs_trgt= evaluateRandomly(encoder, decoder,input_lang,output_lang, data_trgt_test)
trgt_test = corpus_bleu(targets_trgt, outputs_trgt )

print("Bley score for target domain is {} and for source domain is {}".format(trgt_test,src_test ))
