# -*- coding: utf-8 -*-
"""Transformer_MT_Base_NFKC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12F8ltvhlJKIn0v_2gDSjhBnUWa7QzDMx
"""

# !sudo echo -ne '\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing
# !sudo apt update >/dev/null 2>&1
# !sudo apt install google-drive-ocamlfuse >/dev/null 2>&1
# !google-drive-ocamlfuse
# !sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser
# !xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser
# %cd /content
# !mkdir gdrive
# %cd gdrive
# !mkdir "My Drive"
# !google-drive-ocamlfuse "/content/gdrive/My Drive"

# from google.colab import drive
# drive.mount('/content/drive')

# # !pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive"

# !pip install torch
# 
import torch

# -*- coding: utf-8 -*-
"""Transformer_batch_size_arabert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKjYDLWxlvcf-zuNLSXj0c8JgvfOD2ey
"""

from io import open
import unicodedata
import re
import random
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from transformers import ( AutoModel, AutoTokenizer)
import numpy as np
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from torch import Tensor
import torch
import torch.nn as nn
from torch.nn import Transformer
import math
from timeit import default_timer as timer
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CosineAnnealingLR

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import pandas as pd
from sklearn.model_selection import train_test_split
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "aubmindlab/bert-base-arabertv02-twitter"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_name)





# data_vocab = './data_vocab'
# # data_train_file = "./data_Nile_Basin_preprocessed_train"
# data_valid_file = "data_Nile_Basin_preprocessed_valid"
# data_valid= pd.read_csv("./data_Nile_Basin_preprocessed_valid", sep="\t")
# data_test= pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
SOS_token = tokenizer.cls_token_id
EOS_token = tokenizer.sep_token_id
PAD_token = tokenizer.pad_token_id

domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
data_train =[]
for d in domain:
  data = pd.read_csv("./data_{}_preprocessed_train".format(d), sep="\t")
  data_train.append(data)

data_1 = pd.concat([data_train[0], data_train[1]], ignore_index = True)
data_2 = pd.concat([data_train[2], data_train[3]], ignore_index = True)

data_train_src = pd.concat([data_1, data_2], ignore_index = True)
data_train_all =pd.concat([data_train_src, data_train[4]], ignore_index = True)

data = pd.read_csv("./data_North_Africa_preprocessed_train", sep="\t")

print(data_train[0].shape[0]+ data_train[1].shape[0] + data_train[2].shape[0]+data_train[3].shape[0]+data_train[4].shape[0])

data_train_src.shape

domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
data_test =[]
for d in domain:
  data = pd.read_csv("./data_{}_preprocessed_test".format(d), sep="\t")
  data_test.append(data)

data_1 = pd.concat([data_test[0], data_test[1]], ignore_index = True)
data_2 = pd.concat([data_test[2], data_test[3]], ignore_index = True)

data_test_src = pd.concat([data_1, data_2], ignore_index = True)
data_test_all =pd.concat([data_test_src, data_test[4]], ignore_index = True)

domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
data_valid =[]
for d in domain:
  data = pd.read_csv("./data_{}_preprocessed_valid".format(d), sep="\t")
  data_valid.append(data)

data_1 = pd.concat([data_valid[0], data_valid[1]], ignore_index = True)
data_2 = pd.concat([data_valid[2], data_valid[3]], ignore_index = True)

data_valid_src = pd.concat([data_1, data_2], ignore_index = True)
data_valid_all =pd.concat([data_valid_src, data_valid[4]], ignore_index = True)

data_1 = pd.concat([data_valid_all, data_train_all], ignore_index = True)
data_all = pd.concat([data_test_all, data_1], ignore_index = True)

data_vocab = []
# for i in range(len(domain)):
#   data_1 = pd.concat([data_valid[i], data_train[i]], ignore_index = True)
#   data = pd.concat([data_test[i], data_1], ignore_index = True)
#   data_vocab.append(data)

data_vocab_all = data_all

data = pd.read_csv("./data_domian_cosine_arabert_all", sep ="\t")
data_train_1 =[]

for i in range(5):
    data_train_1.append(data.sort_values("score_{}".format(domain[i]), ascending = False))
    data_train_1[i] = data_train_1[i][["source_lang_{}".format(domain[i]), "target_lang"]]
    data_train_1[i].rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)




data_all_1 = pd.concat([data_train_1[0][:5000], data_train_1[1][:5000]], ignore_index = True)
data_all_2 = pd.concat([data_train_1[2][:5000], data_train_1[3][:5000]], ignore_index = True)
data_all_3 = pd.concat([data_all_2, data_all_1], ignore_index = True)
data_all_4 = pd.concat([data_all_3, data_train_1[4][:5000]], ignore_index = True)

# data_all_4.drop_duplicates(inplace = True)
# data_all_4.shape

data_train_all_BT = pd.concat([data_train_all, data_all_4], ignore_index = True)
print(data_train_all.shape, data_all_4.shape)

data_train_all_BT_src  = pd.concat([data_train_src, data_all_3], ignore_index = True)
data_vocab_all_BT = pd.concat([data_vocab_all, data_all_4], ignore_index = True)
data_vocab_all_BT.shape

data_vocab_all_BT_src = pd.concat([data_vocab_all, data_all_3], ignore_index = True)
print(data_train_all_BT.shape)
# data_select = pd.read_csv("./data_selected_sent_arabert_2", sep="\t")
# data_select.shape

# data_train = pd.read_csv("./data_Nile_Basin_preprocessed_train", sep="\t")
# # data_select = pd.read_csv("./data_finetune_score_LORA", sep="\t")
# data_select = pd.read_csv("./data_selected_sent_arabert_2", sep="\t")

# data_select.dropna(inplace=True)
# for idx in data_select.index:
#   sent = data_select["target_lang"][idx]
#   print(sent,idx)
#   z= re.findall('\d+', sent)
#   if z:
#     data_select=data_select.drop([idx])
# data_train_2 = data_select.sort_values("score", ascending = False, inplace = True)
# data_train_2 =data_select[:5000]
# data_train_2 = data_train_2[["target_lang","source_lang"] ]
# data_train = data_train [["target_lang","source_lang"] ]
# data_train_2.shape

# df = pd.concat([data_train, data_train_2], ignore_index= True)
# df.head()

# df.to_csv("./data_Nile_basin_BT_sent_arabert_2", sep="\t")
# data_train_file = "./data_Nile_basin_BT_sent_arabert_2"

#determine i
# i= 2
# z= 2
# sent_bert data

# data = pd.read_csv("./data_selected_domain_finetune_arabert_large_MADAR_2", sep ="\t")



# data = pd.read_csv("./data_domian_cosine_sent_bert_all", sep ="\t")
# sent_arabert data trained on 5 epochs
# data = pd.read_csv("./data_domian_cosine_sent_arabert_large_all", sep ="\t")
# arabert data

# data_selected_domain_finetune_arabert_large
# data_selected_domain_finetune_arabert_large_all
# data.to_csv("./data_selected_domain_finetune_arabert_large_{}".format(domain[i]), sep ="\t")
# data.columns

# # sent_arabert data trained on 10 epochs
# data = pd.read_csv("./data_domian_cosine_sent_arabert_large_all_2", sep ="\t")

# domain finetune sorting
# data_1 = data.loc[data["prediction"]==1].sort_values("prob",ascending = False)
# data_2 = data.loc[data["prediction"]==0].sort_values("prob",ascending = True)

# data_finetune = pd.concat([data_1, data_2[:(20000 - data_1.shape[0])]])



# domain cosine sorting
# data_1 = data.sort_values("score_{}".format(domain[i]), ascending = False)

# data_1 = data_1[:20000]
# data_finetune = data_1[["source_lang_{}".format(domain[i]), "target_lang"]]
# data_finetune.rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)
# data_train_BT = pd.concat([data_train[i], data_finetune], ignore_index = True)


data_train_BT= data_train[i]
print(data_train_BT.shape)

data_train[3].shape
class Lang:
    def __init__(self, name):
        self.name = name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.word2index = { self.tokenizer.pad_token : self.tokenizer.pad_token_id}
        self.word2count = {}
        self.index2word = {self.tokenizer.pad_token_id: self.tokenizer.pad_token}
        self.n_words = 1 # Count PAD token

    def addSentence(self, sentence):
        for word in self.tokenizer.tokenize(sentence, add_special_tokens= True):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.tokenizer.convert_tokens_to_ids(word)
            self.word2count[word] = 1
            self.index2word[self.tokenizer.convert_tokens_to_ids(word)] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

# Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFKC', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters


def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([?.!,¿])", r" \1 ", s)
    s = re.sub(r'[" "]+', " ", s)

    s = re.sub(r"[^a-zA-Z؀-ۿ?.!,¿]+", " ", s)
    s = re.sub(r"([.!?])", r" \1", s)
    # s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

def readLangs(lang1, lang2, reverse=False, label ="train"):
    print("Reading lines...")

    # Read the file and split into lines
    if label=="vocab":
        # lines = open(data_vocab, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_vocab_all_BT_src
    if label =="train":
        # lines = open(data_train_file, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_train_all_BT_src
    if label =="valid":
        # lines = open(data_valid_file, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_valid_src
    # lines = open('/content/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\
    #     read().strip().split('\n')

    # Split every line into pairs and normalize
    pairs = [[normalizeString(data.source_lang[idx]), normalizeString(data.target_lang[idx])] for idx in data.index]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

MAX_LENGTH = 200
def prepareData(lang1, lang2, reverse=False, label="train"):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, label = label)
    print("Read %s sentence pairs" % len(pairs))
    # pairs = filterPairs(pairs)
    # print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    pairs = pairs[1:]
    for pair in pairs:
        input_lang.addSentence(pair[0])
        input_lang.addSentence(pair[1])
    output_lang = input_lang
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs


# input_lang, output_lang, pairs = prepareData('Cairo', 'MSA')



def indexesFromSentence(lang, sentence):
    return [lang.word2index.get(word,0) for word in tokenizer.tokenize(sentence, add_special_tokens=True)]

def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)

def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

def get_dataloader(batch_size, label ="train" ):
    input_lang, output_lang, _ = prepareData('ar', 'arz', label="vocab")
    _, _, pairs = prepareData('ar', 'arz', label = label)
    n = len(pairs)
    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)
    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)

    for idx, (inp, tgt) in enumerate(pairs):
        inp_ids = indexesFromSentence(input_lang, inp)
        tgt_ids = indexesFromSentence(output_lang, tgt)
        # inp_ids.append(EOS_token)
        # tgt_ids.append(EOS_token)
        input_ids[idx, :len(inp_ids)] = inp_ids
        target_ids[idx, :len(tgt_ids)] = tgt_ids

    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),
                               torch.LongTensor(target_ids).to(device))

    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
    return input_lang, output_lang, train_dataloader

# input_lang, output_lang, train_dataloader = get_dataloader(32)

# data = iter(train_dataloader)
# next(data)[0]

# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.
class PositionalEncoding(nn.Module):
    def __init__(self,
                 emb_size: int,
                 dropout: float,
                 maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den) # is an indexing operation that targets every other
                                                      #column of the pos_embedding tensor, starting from
                                                      #the second column (index 1) and going up to the last
                                                      #column (index emb_size-1).
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

# helper Module to convert tensor of input indices into corresponding tensor of token embeddings
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

# Seq2Seq Network
class Seq2SeqTransformer(nn.Module):
    def __init__(self,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 emb_size: int,
                 nhead: int,
                 src_vocab_size: int,
                 tgt_vocab_size: int,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1):
        super(Seq2SeqTransformer, self).__init__()
        self.transformer = Transformer(d_model=emb_size,
                                       nhead=nhead,
                                       num_encoder_layers=num_encoder_layers,
                                       num_decoder_layers=num_decoder_layers,
                                       dim_feedforward=dim_feedforward,
                                       dropout=dropout)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)

        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(
            emb_size, dropout=dropout)

    def forward(self,
                src: Tensor,
                trg: Tensor,
                src_mask: Tensor,
                tgt_mask: Tensor,
                src_padding_mask: Tensor,
                tgt_padding_mask: Tensor,
                memory_key_padding_mask: Tensor):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src: Tensor, src_mask: Tensor, src_padding_mask):
        return self.transformer.encoder(self.positional_encoding(
                            self.src_tok_emb(src)), src_mask, src_key_padding_mask  = src_padding_mask)

    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor, tgt_padding_mask):
        return self.transformer.decoder(self.positional_encoding(
                          self.tgt_tok_emb(tgt)), memory,
                          tgt_mask, tgt_key_padding_mask = tgt_padding_mask)

def generate_square_subsequent_mask(sz):
    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

PAD_IDX = 0
def create_mask(src, tgt):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]

    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)

    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

def train_epoch(model, optimizer ):
    model.train()
    losses = 0
    valid_losses =0

    start_time = timer()
    for src, tgt in train_dataloader:
        src = src.to(device)
        tgt = tgt.to(device)

        inp_len = [ len(src[i][src[i]!=0]) for i in range(src.shape[0])]
        trgt_len = [len(tgt[i][tgt[i]!=0]) for i in range(tgt.shape[0]) ]
        input_tensor = src[:, :max(inp_len)]
        target_tensor = tgt[:, :max(trgt_len)]
        src = input_tensor.T.to(device)
        tgt = target_tensor.T.to(device)
        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)

        optimizer.zero_grad()

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()

        optimizer.step()
        # scheduler.step()
        losses += loss.item()
    end_time = timer()
    model.eval()
    valid_start_time = timer()
    for src, tgt in valid_dataloader:
        # Move batch to device
        src = src.to(device)
        tgt = tgt.to(device)

        inp_len = [ len(src[i][src[i]!=0]) for i in range(src.shape[0])]
        trgt_len = [len(tgt[i][tgt[i]!=0]) for i in range(tgt.shape[0]) ]
        input_tensor = src[:, :max(inp_len)]
        target_tensor = tgt[:, :max(trgt_len)]
        src = input_tensor.T.to(device)
        tgt = target_tensor.T.to(device)
        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
        with torch.no_grad():
          logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)



        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))

        valid_losses +=loss.item()
    valid_end_time = timer()




    scheduler.step(losses / len(list(train_dataloader)))
    #scheduler.step()

    return   transformer, optimizer, start_time, end_time, valid_start_time, valid_end_time, losses / len(list(train_dataloader)), valid_losses /  len(list(valid_dataloader))

torch.cuda.empty_cache()

model ={}
best_loss = - np.inf



NUM_EPOCHS = 100
# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 =NUM_EPOCHS , T_mult=1, verbose = True)
torch.cuda.empty_cache()
# model={1: transformer, 2: transformer, 3: transformer}
# experiments = 3


# for i in [1]:
    # data_1 = data.sort_values("score_{}".format(domain[i]), ascending = False)
    # data_1 = data_1[:1000]
    # data_finetune = data_1[["source_lang_{}".format(domain[i]), "target_lang"]]
    # data_finetune.rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)
    # data_train_BT = pd.concat([data_train[i], data_finetune], ignore_index = True)

batch_size = 16
input_lang, output_lang, train_dataloader = get_dataloader(batch_size = batch_size, label ="train")
_, _, valid_dataloader = get_dataloader(batch_size = batch_size, label ="valid")



torch.cuda.empty_cache()
# NUM_EPOCHS = 100
# best_valid_loss = 1000
# early_stop_threshold = 3
# best_epoch =-1
# j=0

# torch.manual_seed(0)
# SRC_VOCAB_SIZE = tokenizer.vocab_size
# TGT_VOCAB_SIZE = tokenizer.vocab_size
# EMB_SIZE = 512
# NHEAD = 2
# FFN_HID_DIM = 512

# NUM_ENCODER_LAYERS = 2
# NUM_DECODER_LAYERS = 2


# transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
#                           NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
# for p in transformer.parameters():
#     if p.dim() > 1:
#         nn.init.xavier_uniform_(p)
# transformer = transformer.to(DEVICE)
# loss_fn = nn.CrossEntropyLoss()
# optimizer = optim.Adam(transformer.parameters(), lr=0.0001)

# scheduler = ReduceLROnPlateau(optimizer, patience=2, verbose = True)


for z in [1,2]:
    torch.cuda.empty_cache()
    NUM_EPOCHS = 100
    best_valid_loss = 1000
    early_stop_threshold = 3
    best_epoch =-1
    j=0
    
    torch.manual_seed(z)
    SRC_VOCAB_SIZE = tokenizer.vocab_size
    TGT_VOCAB_SIZE = tokenizer.vocab_size
    # EMB_SIZE = 512
    # NHEAD = 2
    # FFN_HID_DIM = 512

    # NUM_ENCODER_LAYERS = 2
    # NUM_DECODER_LAYERS = 2
    
    # EMB_SIZE = 512
    # NHEAD = 8
    # FFN_HID_DIM = 1024

    # NUM_ENCODER_LAYERS = 6
    # NUM_DECODER_LAYERS = 6
    
    # EMB_SIZE = 512
    # NHEAD = 4
    # FFN_HID_DIM = 1024

    # NUM_ENCODER_LAYERS = 4
    # NUM_DECODER_LAYERS = 4
    
    
    EMB_SIZE = 512
    NHEAD = 8
    FFN_HID_DIM = 1024

    NUM_ENCODER_LAYERS = 2
    NUM_DECODER_LAYERS = 2
    
    transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                              NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    transformer = transformer.to(DEVICE)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(transformer.parameters(), lr=0.0001)
    
    scheduler = ReduceLROnPlateau(optimizer, patience=2, verbose = True)
    
    # print("loading ...")
    # checkpoint = torch.load("./checkpoint_transformer_base_paper_BT_FF1024_EDL4_{}.pt".format(z))
    
    # transformer.load_state_dict(checkpoint['model_state_dict'])
    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    for epoch in range(1, NUM_EPOCHS+1):
    
        if j < early_stop_threshold:
            transformer, optimizer,  start_time, end_time, valid_start_time, valid_end_time, train_loss, valid_loss = train_epoch( transformer, optimizer )
    
            if (valid_loss < best_valid_loss ):
              best_model = transformer
              best_epoch = epoch
              best_valid_loss = valid_loss
              print("Saving ...")
              torch.save({
    
                    'model_state_dict': transformer.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
    
                    }, "./checkpoint_transformer_UDA_sorce_domain_paper_BT5000_FF1024_AH8_EDL2_{}.pt".format(z))
              j = 0
    
            else:
              j =j+1
    
            print((f"Epoch: {epoch}, Train loss: {train_loss:.9f}, "f"Epoch time = {(end_time - start_time)/60:.3f}m"),
                    (f"Validation loss: {valid_loss:.9f}, "f"Epoch time = {(valid_end_time - valid_start_time)/60:.3f}m") )
        else:
          break

def greedy_decode(model, src, src_mask,src_padding_mask, max_len, start_symbol, output_lang):
    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)
    decoded_words = []
    with torch.no_grad():
        memory = model.encode(src, src_mask,src_padding_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        tgt_padding_mask = (ys == PAD_IDX).transpose(0, 1)
        out = model.decode(ys, memory, tgt_mask,tgt_padding_mask )
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        decoded_words.append(output_lang.index2word.get(next_word,output_lang.index2word[1]))
        if next_word == EOS_token:
            break
    return decoded_words, ys

def decode(model, src, src_mask,src_padding_mask, max_len, start_symbol, output_lang,
             p=None, greedy=None):
    """ Main decoding function, beam search is in a separate function """

    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)
    decoded_words = []
    with torch.no_grad():
        memory = model.encode(src, src_mask,src_padding_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        tgt_padding_mask = (ys == PAD_IDX).transpose(0, 1)
        out = model.decode(ys, memory, tgt_mask,tgt_padding_mask )
        out = out.transpose(0, 1)
        logits = model.generator(out[:, -1])
        prob = F.softmax(logits, dim=-1)
        logprobs = F.log_softmax(logits, dim=-1)

        if greedy:
          _, next_word = torch.max(prob, dim=1)
          next_word = next_word.item()


        if p is not None:

          sorted_probs, sorted_indices = torch.sort(prob, descending=True)
          cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
          sorted_indices_to_remove = cumulative_probs > p
          sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
          sorted_indices_to_remove[:, 0] = 0
          sorted_samp_probs = sorted_probs.clone()
          sorted_samp_probs[sorted_indices_to_remove] = 0



          sorted_next_indices = sorted_samp_probs.multinomial(1).view(-1, 1)

          next_tokens = sorted_indices.gather(1, sorted_next_indices)
          next_word = next_tokens.item()

          next_logprobs = sorted_samp_probs.gather(1, sorted_next_indices).log()

        ys = torch.cat([ys,
                          torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        decoded_words.append(output_lang.index2word.get(next_word,output_lang.index2word[1]))
        if next_word == EOS_token:
            break
    return decoded_words, ys

# actual function to translate input sentence into target language
def translate(model: torch.nn.Module, data, input_lang, output_lang, n=None):
    targets =[]
    outputs =[]
    model.eval()
    i=0
    for idx in data.index:
      src_sentence = data.source_lang[idx]
      trgt = data.target_lang[idx]
      if i == n:
          break

      src = tensorFromSentence(input_lang, normalizeString(src_sentence)).view(-1, 1)
      num_tokens = src.shape[0]
      src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
      src_padding_mask = (src == PAD_IDX).transpose(0, 1)

      decoded_words, _ = decode(
          model,  src, src_mask,src_padding_mask,max_len=num_tokens + 5,
          start_symbol=SOS_token, output_lang=output_lang, greedy = True
          #p=0.95
          )
      output_words = tokenizer.convert_tokens_to_string(decoded_words)
      output_words = output_words.replace("[SEP]", "")
      output_words = output_words.replace("[UNK]", "")
      output_words = output_words.replace("[PAD]", "")
      # print("input: ", src_sentence)
      # print("target: ", trgt)
      # print("prediction: ", output_words)
      # print('')
      i=i+1
      targets.append([normalizeString(trgt)])
      outputs.append(output_words)
    return outputs, targets

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive



from nltk.translate.bleu_score import corpus_bleu

for i in [1]:
    bleu={
        "Gulf": [],
            "Iraqi": [],
            "Levantine":[],
            "Nile_Basin":[],
            "North_Africa":[]
          }
    
    # model_path_list = ["./checkpoint_transformer_{}_{}.pt".format(domain[i],x) for x in [0,1,2]]
    model_path_list = ["./checkpoint_transformer_UDA_sorce_domain_paper_BT5000_FF1024_AH8_EDL2_{}.pt".format(x)for x in [1,2]]
   
    
   
    # checkpoint = torch.load("./checkpoint_transformer_finetune_arabert_large_MADAR_2_{}_BT_4_2_{}.pt".format(domain[i],z))
    z=0
    
    # model_path_list= ["checkpoint_transformer_Gulf.pt"]
    for path in model_path_list:
        
        torch.manual_seed(z)
        z +=1
        transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
        for p in transformer.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        transformer = transformer.to(DEVICE)
        loss_fn = nn.CrossEntropyLoss()
        optimizer = optim.Adam(transformer.parameters(), lr=0.0001)
        checkpoint = torch.load(path)
        print("loading")
        transformer.load_state_dict(checkpoint['model_state_dict'])
    
    
        for j, (k, v) in enumerate(bleu.items()):
        
            data = data_test[j]
            outputs, target= translate(transformer, data, input_lang, output_lang)
            src_test = corpus_bleu(target, outputs)
            bleu[k].append(src_test)
            print(bleu)
            src_test



# checkpoint_transformer_UDA_sorce_domain_paper_BT5000_FF1024_AH8_EDL2_, we tested BT with 5000 sentences from src domain (resulting in a total of 20000 (4*5000) sentences)
# {'Gulf': [0.597692287071884,0.6120891477712984, 0.602695063619791 ], 'Iraqi': [0.7327885490200492, 0.7484878639008752, 0.7431371814237584], 'Levantine': [0.5563647864235904, 0.570837285781338, 0.56371365448938], 'Nile_Basin': [0.6024808825091852,0.6171075155887564, 0.6117085135476052], 'North_Africa': [0.31006419113366196, 0.30788594836848027, 0.3140898065682471]}
# {'Gulf': [0.6120891477712984, 0.602695063619791], 'Iraqi': [0.7484878639008752, 0.7431371814237584], 'Levantine': [0.570837285781338, 0.56371365448938], 'Nile_Basin': [0.6171075155887564, 0.6117085135476052], 'North_Africa': [0.30788594836848027, 0.3140898065682471]}

# checkpoint_transformer_UDA_sorce_domain_paper_BT_FF1024_AH8_EDL2_ , source domain, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 2 NUM_DECODER_LAYERS = 2, 
# {'Gulf': [0.6054117541518728, 0.5938975687446467, 0.5962358160031159], 'Iraqi': [0.7281274929798414, 0.7271356903577093, 0.7263881073961196], 'Levantine': [0.5683099728499966, 0.5577727536269005, 0.5613853073761427], 'Nile_Basin': [0.6076703042472588, 0.6082722191768715, 0.6042972095342416], 'North_Africa': [0.30511795455007135, 0.2963522447641034, 0.2952855732111573]}

# without BT checkpoint_transformer_UDA_sorce_domain_paper_FF1024_AH8_EDL2_ , source domain, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 2 NUM_DECODER_LAYERS = 2, 
# {'Gulf': [0.5972717001758135, 0.5982771664359354, 0.6034422012067989], 'Iraqi': [0.7709266520650333, 0.7547018598964822, 0.7609667436424117], 'Levantine': [0.5621476509454068, 0.5554040486371232, 0.5661200614711065], 'Nile_Basin': [0.6097261039412042, 0.6066091310921086, 0.6092744869508784], 'North_Africa': [0.3108407161140688, 0.29926032535204045, 0.31036307763415577]}


# checkpoint_transformer_base_paper_BT_FF10240 this is trained for 13 epochs
# Multi_dialect BT model trained using EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 6, NUM_DECODER_LAYERS = 6, ES=512, FF=1024, nhead= 8, EL =6, DL=6,  
# checkpoint_transformer_base_paper_BT_FF10240
# {'Gulf': [0.5571282779064498], 'Iraqi': [0.7031176985674563], 'Levantine': [0.5184902375291024], 'Nile_Basin': [0.5600339120546357], 'North_Africa': [0.4513492138157906]}


# checkpoint_transformer_base_paper_BT_FF1024_EDL4_0, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 4, NUM_DECODER_LAYERS = 4
# {'Gulf': [0.6390167811891979], 'Iraqi': [0.7639709712939322], 'Levantine': [0.6141370891753279], 'Nile_Basin': [0.6484840717377246], 'North_Africa': [0.536170703946218]}

# checkpoint_transformer_base_paper_BT_FF1024_AHEDL4_0, EMB_SIZE = 512 , NHEAD = 4, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 4, NUM_DECODER_LAYERS = 4
# {'Gulf': [0.6391811046907562], 'Iraqi': [0.7536000429619456], 'Levantine': [0.6075897670469727], 'Nile_Basin': [0.6433514503263525], 'North_Africa': [0.537215556239612]}

# checkpoint_transformer_base_paper_BT_FF1024_AH8_EDL2_0, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 2, NUM_DECODER_LAYERS = 2
# {'Gulf': [0.6402131377310399], 'Iraqi': [0.7533898830167444], 'Levantine': [0.6182785180691263], 'Nile_Basin': [0.6519933903982543], 'North_Africa': [0.5422994200814593]}

# checkpoint_transformer_base_paper_BT_EDFF1024_AH8_EDL2_00, EMB_SIZE = 1024 , NHEAD = 8, FFN_HID_DIM = 1024, NUM_ENCODER_LAYERS = 2, NUM_DECODER_LAYERS = 2
# {'Gulf': [0.6218267675802535], 'Iraqi': [0.7515078950211833], 'Levantine': [0.6022805465401031], 'Nile_Basin': [0.6270433279856882], 'North_Africa': [0.5269618944343644]}

# checkpoint_transformer_base_paper_BT_EDFF512_AH8_EDL2_0, EMB_SIZE = 512 , NHEAD = 8, FFN_HID_DIM = 512, NUM_ENCODER_LAYERS = 2, NUM_DECODER_LAYERS = 2
# {'Gulf': [0.6441664541886163], 'Iraqi': [0.7367288755291307], 'Levantine': [0.608604797979947], 'Nile_Basin': [0.6369445022847765], 'North_Africa': [0.5305925393877499]}



# base_model with BT for source domain, all, checkpoint_transformer_UDA_base_BT_0,1,2.pt
# {'Gulf': [0.5894374444476336, 0.5947829009169366, 0.5777648771893217], 'Iraqi': [0.7052802494627243, 0.7088465623828415, 0.702550310463655], 'Levantine': [0.5505838938451393, 0.5628277527659523, 0.5479533445537093], 'Nile_Basin': [0.5907274419230469, 0.6042037863807745, 0.5898945106936381], 'North_Africa': [0.2967718227233345, 0.29833520326917046, 0.29389457539225117]}



# base_model without BT for source domain, all, checkpoint_transformer_UDA_base_0,1,2.pt
# {'Gulf': [0.5939609674406768, 0.5870850926482667, 0.580940017044124], 'Iraqi': [0.7534491376722473, 0.7342764336179748, 0.7300846230446586], 'Levantine': [0.5618099367887162, 0.5486210656431958, 0.5460161894118924], 'Nile_Basin': [0.6027746380586632, 0.5974342373306974, 0.5823087977374283], 'North_Africa': [0.3027583772678766, 0.30269599168004524, 0.30039768026402225]}



# base_model, all, checkpoint_transformer_all_0
# {'Gulf': [0.6236590130161723, 0.6346136881386315, 0.6307457749631864], 'Iraqi': [0.7707909616220588, 0.7676510967756214, 0.7665113574670482], 'Levantine': [0.6018629621162663, 0.6108405519818553, 0.6179157305846837], 'Nile_Basin': [0.6339468483169184, 0.6333368753721657, 0.6432062731155107], 'North_Africa': [0.5292512507316905, 0.5356525935066878, 0.5435497461270108]}
#Multi_dialect BT, checkpoint_transformer_all_BT_0,1,2.pt
# {'Gulf': [0.640571410640815, 0.6350791785668616, 0.6391987748425572], 'Iraqi': [0.7380603849003454, 0.7376338278165184, 0.7400480277638103], 'Levantine': [0.6145312935172308, 0.6018124710768219, 0.6144853785163675], 'Nile_Basin': [0.6478756202391289, 0.6286806639253821, 0.6439247308739886], 'North_Africa': [0.5404940768675348, 0.5275987697657762, 0.5374938923085568]}



#Gulf base model, checkpoint_transformer_Gulf_0,1,2.pt
# {'Gulf': [0.40282858546908573, 0.46478151508868115, 0.4209435999641955], 'Iraqi': [0.3614396039950725, 0.431222900303736, 0.37584410710641053], 'Levantine': [0.24368183328110227, 0.26864312615688496, 0.24729997373633614], 'Nile_Basin': [0.2991528169006522, 0.33760453602132734, 0.3091940648707596], 'North_Africa': [0.19375701412205212, 0.20426502336900182, 0.19630434964763513]}


#Iraqi base model, checkpoint_transformer_Iraqi_0,1,2.pt
# {'Gulf': [0.29936831408772663, 0.3031552929272073, 0.293896867246292], 'Iraqi': [0.5248592740710422, 0.5191278602386913, 0.5166402351060324], 'Levantine': [0.23919823018115355, 0.2331202118805104, 0.2353028840978769], 'Nile_Basin': [0.2567596338957787, 0.2507100884474116, 0.2424987392115867], 'North_Africa': [0.17991655638135817, 0.17609276849766262, 0.1789663807204496]}
# Iraqi BT, domain cosine Arabert, 20,000
# {'Gulf': [0.33196556238584923, 0.3261389045145726, 0.3238971137033049], 'Iraqi': [0.4812534022049431, 0.47331073598159695, 0.4637252393655868], 'Levantine': [0.25931834815476007, 0.25720963540594843, 0.25504457015563], 'Nile_Basin': [0.2801137579163619, 0.276289635924348, 0.27789304968886475], 'North_Africa': [0.207467458747485, 0.20790595207413415, 0.21089793560800943]}
# Iraqi BT, domain cosine Arabert, 5,000
# {'Gulf': [0.31208241546745413, 0.3224653087092137, 0.29932182881971303], 'Iraqi': [0.481832946471559, 0.48484657337717973, 0.4509596148593388], 'Levantine': [0.23065848007255632, 0.23400509236349235, 0.22964593123903046], 'Nile_Basin': [0.2554812898049006, 0.26216651985766504, 0.2559034724429531], 'North_Africa': [0.1810354343115221, 0.18419595986006135, 0.1842637827587589]}



#Levantine base model, checkpoint_transformer_Levantine_0,1,2.pt
# {'Gulf': [0.3645588234867576, 0.375908486737892, 0.3709254236553951], 'Iraqi': [0.4164884213079568, 0.4372378657427753, 0.43491931929589295], 'Levantine': [0.43913784472723055, 0.4778648278512015, 0.4584773988053355], 'Nile_Basin': [0.339944174153126, 0.36030586490819844, 0.3609334214376995], 'North_Africa': [0.22326514358183622, 0.23200755858411803, 0.2352299429761898]}
# Levantine BT, domain cosine Arabert, 
# {'Gulf': [0.39594733444976266, 0.39953146447781845, 0.3903399882626562], 'Iraqi': [0.43490192666579974, 0.43398277941529784, 0.4129750621932066], 'Levantine': [0.480443826108278, 0.48777270736689976, 0.4804257090508729], 'Nile_Basin': [0.3978447808170201, 0.40483664363814176, 0.38545031936908286], 'North_Africa': [0.26909441813762847, 0.26893559761000274, 0.266653848642084]}


#Nile_Basin base model, checkpoint_transformer_Nile_Basin_0,1,2.pt
# {'Gulf': [0.29749318468048724, 0.32862334401584387, 0.3091317040622198], 'Iraqi': [0.2945207112389185, 0.32966692866411884, 0.33851221266464854], 'Levantine': [0.24608971222098547, 0.2646774898675002, 0.2607693218076268], 'Nile_Basin': [0.4092846590981971, 0.44457598152489464, 0.4366229371026178], 'North_Africa': [0.19775145334944422, 0.19936134930031385, 0.2041096372146791]}
# Nile_Basin BT, domain cosine Arabert, 
# {'Gulf': [0.3554446453932299, 0.3521906643361832, 0.3473161377790615], 'Iraqi': [0.33887892861464736, 0.32833452075286323, 0.3270642353124871], 'Levantine': [0.2875475154927324, 0.2897757849167907, 0.2811091531162784], 'Nile_Basin': [0.4783381535997286, 0.4642467069542553, 0.4722783183469496], 'North_Africa': [0.23703632498730884, 0.2364628395788239, 0.2341678662646506]}

#North_Africa base model, checkpoint_transformer_North_Africa_0,1,2.pt
# {'Gulf': [0.3744005383870448, 0.37982856111425467, 0.37490926819789744], 'Iraqi': [0.3832940740806156, 0.3941997040112098, 0.39146481326973853], 'Levantine': [0.3093303626916332, 0.3170929489836701, 0.3162624467169962], 'Nile_Basin': [0.34783877193158985, 0.359392556821936, 0.3680438055857777], 'North_Africa': [0.4375272240704752, 0.46352816624000165, 0.45009759709629726]}
# North_Africa BT, domain cosine Arabert, 
# {'Gulf': [0.382657466050382, 0.3885040056401269, 0.40421381967157677], 'Iraqi': [0.3735050455118588, 0.38610460855109097, 0.40026668843167545], 'Levantine': [0.3160992155075425, 0.31966867975135405, 0.3330340087494579], 'Nile_Basin': [0.36258014132736466, 0.3713094463629199, 0.3812451065368346], 'North_Africa': [0.4572658788557762, 0.4629467193473643, 0.469766714718869]}


#Gulf model trained using 20000 new sentences, seed=0, domain cosine sent_bert BT_4, 
# {'Gulf': [0.46658280962327553], 'Iraqi': [0.38194214653092196], 'Levantine': [0.28444467202280543], 'Nile_Basin': [0.3515410835813963], 'North_Africa': [0.2258423387705379]}
# seed=1, domain cosine sent_bert BT_4, "./checkpoint_transformer_sent_bert_Gulf_BT_4_1.pt"
# {'Gulf': [0.4672313214981059], 'Iraqi': [0.3952048976914266], 'Levantine': [0.2882549906077306], 'Nile_Basin': [0.3527302196457813], 'North_Africa': [0.22958644761237199]}
# seed=2, domain cosine sent_bert BT_4, "./checkpoint_transformer_sent_bert_Gulf_BT_4_2.pt"
# {'Gulf': [0.4587474685599565], 'Iraqi': [0.38451918512705074], 'Levantine': [0.27648761068970706], 'Nile_Basin': [0.339380522506113], 'North_Africa': [0.2178999119081497]}



#Gulf model trained using 20000 new sentences, seed=0, domain cosine arabert_large BT_4, checkpoint_transformer_Gulf_BT_4.pt
# {'Gulf': [0.4687816662831769], 'Iraqi': [0.3986859545458349], 'Levantine': [0.29039207648888193], 'Nile_Basin': [0.3595923339090198], 'North_Africa': [0.2299841030648122]}
#Gulf model trained using 20000 new sentences,  seed=1,domain cosine arabert_large BT_4, checkpoint_transformer_Gulf_BT_4_1.pt
# {'Gulf': [0.4716511271022372], 'Iraqi': [0.4038477012872079], 'Levantine': [0.2915560789756915], 'Nile_Basin': [0.354813366740185], 'North_Africa': [0.23447923181875233]}
#Gulf model trained using 20000 new sentences,  seed=2,domain cosine arabert_large BT_4, checkpoint_transformer_Gulf_BT_4_2.pt
# {'Gulf': [0.46073961643777306], 'Iraqi': [0.4010952811952371], 'Levantine': [0.2919608864944798], 'Nile_Basin': [0.3573007863423699], 'North_Africa': [0.23126517801173843]}


#Gulf model trained using 20000 new sentences, domain finetune using arabert_large_MADAR BT_4, checkpoint_transformer_finetune_arabert_large_MADAR_2_Gulf_BT_4_2_0.pt, finetune trained on 10 epochs

# {'Gulf': [0.4669074013953363, 0.4618795575173086, 0.45864671004062985], 'Iraqi': [0.40133009511235224, 0.38727115173464943, 0.3990940085919328], 'Levantine': [0.2970069570675032, 0.28980422917550924, 0.28293425219902685], 'Nile_Basin': [0.35634561700632744, 0.3555783154224251, 0.3491818615721882], 'North_Africa': [0.23106222247844552, 0.22824255559258277, 0.22721386939368443]}




#Gulf model trained using 20000 new sentences, sent_arabert_large BT_4, trained on 10 epochs , checkpoint_transformer_domin_cosine_sent_arabert_large_2_Gulf_BT_4_0.pt
# {'Gulf': [0.4698503171058188], 'Iraqi': [0.40157313386520904], 'Levantine': [0.2894066622246644], 'Nile_Basin': [0.3524572740042148], 'North_Africa': [0.22691029697957102]}
# {'Gulf': [0.4714502090797328, 0.45149772184414333], 'Iraqi': [0.4048869104256865, 0.3876441825081289], 'Levantine': [0.29872110763054893, 0.28864846557342727], 'Nile_Basin': [0.3579928045218911, 0.346931026992486], 'North_Africa': [0.23503223894317257, 0.22859393446618068]}





#Gulf model trained using 20000 new sentences, sent_arabert_large BT_4, trained on 5 epochs,checkpoint_transformer_domin_cosine_sent_arabert_large_Gulf_BT_4.pt
# {'Gulf': [0.4741570180219695], 'Iraqi': [0.39659381967475293], 'Levantine': [0.29914271194986625], 'Nile_Basin': [0.364496771851127], 'North_Africa': [0.2322262029781738]}
# sent_arabert_large BT_4, trained seed =1 ,checkpoint_transformer_domin_cosine_sent_arabert_large_Gulf_BT_4_1.pt
# {'Gulf': [0.46705359365562393], 'Iraqi': [0.3799979063018468], 'Levantine': [0.29246734322296764], 'Nile_Basin': [0.35199526333958586], 'North_Africa': [0.23294626495242107]}
# sent_arabert_large BT_4, trained seed =2 ,checkpoint_transformer_domin_cosine_sent_arabert_large_Gulf_BT_4_2.pt
# {'Gulf': [0.4558599609687854], 'Iraqi': [0.3774414202127735], 'Levantine': [0.28911865076125465], 'Nile_Basin': [0.34614554975265455], 'North_Africa': [0.22881663606911837]}







#Gulf model trained using 20000 new sentences, domain finetune using arabert_large BT_4 which trained using only Gulf target dataset
# {'Gulf': [0.47226364936861936], 'Iraqi': [0.40211533094118806], 'Levantine': [0.2919290240396697], 'Nile_Basin': [0.34675799064131085], 'North_Africa': [0.22615552206198133]}






#Gulf model trained using 20000 new sentences, domain finetune using arabert_large_all BT_4, data finetuned using all target sentences
# {'Gulf': [0.4695606149834618], 'Iraqi': [0.38943577960513476], 'Levantine': [0.28859857450485144], 'Nile_Basin': [0.34831464339275603], 'North_Africa': [0.22877187931406387]}

#Gulf model trained using 10000 new sentences, sent_bert BT_2
# {'Gulf': [0.4400927528643275], 'Iraqi': [0.3753070250810935], 'Levantine': [0.26359180683605027], 'Nile_Basin': [0.3284072872840056], 'North_Africa': [0.21058788853972452]}


#Gulf model trained using 15000 new sentences, arabert_large BT_3
# {'Gulf': [0.46980788338040397], 'Iraqi': [0.39388982595927147], 'Levantine': [0.27965569013421504], 'Nile_Basin': [0.3408641431793468], 'North_Africa': [0.22439803025950558]}
#Gulf model trained using 10000 new sentences, arabert_large BT_2
# {'Gulf': [0.4481861214004342], 'Iraqi': [0.3898832335241325], 'Levantine': [], 'Nile_Basin': [], 'North_Africa': []}




#all models below are wrong, used ascending = true in sort_values
# Models BT_4 , in this order ["Gulf", "Levantine", "Nile_Basin", "North_Africa"], each model trained with 20000 new sentences
# {'Gulf': [0.46596448595289086, 0.4062263851546478, 0.35206119586983275, 0.38526853117458276], 
# 'Iraqi': [0.3964377617069903, 0.41602776368202776, 0.3130742069131432, 0.3777451689983332], 
# 'Levantine': [0.2748937735565306, 0.483545237398735, 0.2849678691248653, 0.3241829116910211], 
# 'Nile_Basin': [0.3447207697292034, 0.3684812038893462, 0.48774587868896313, 0.373556080949612],
 # 'North_Africa': [0.2147959714469623, 0.2437186056179308, 0.23431076665628314, 0.47020521763039275]}


# Model BT Iraqi , model trained with 5000 new sentences
# {'Gulf': [0.29054600741838843], 'Iraqi': [0.48649042456504965], 'Levantine': [0.22761036703217652], 'Nile_Basin': [0.24217505200410702], 'North_Africa': [0.17817481539756944]}

# 0.4484171863981837  Gulf for 5000  new sentences      BT
# 0.44081588040799186 Gulf for 10000 new sentences      BT_2
# 0.46336230667687067 Gulf for 15000 new sentences      BT_3

# 0.46921269987658887 Iraqi for 10000 new sentences BT_2


# 0.46596448595289086 Gulf         for 20000 new sentences  BT_4
# 0.46634138612129106 Iraqi        for 20000 new sentences  BT_4
# 0.483545237398735   Levantine    for 20000 new sentences  BT_4
# 0.48774587868896313 Nile_Basin   for 20000 new sentences  BT_4
# 0.47020521763039275 North_Africa for 20000 new sentences  BT_4




# from nltk.translate.bleu_score import corpus_bleu

# model = ["./checkpoint_transformer_src.pt","./checkpoint_transformer_src_2.pt", "./checkpoint_transformer_src_3.pt"]
# domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]


# data_test_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
# bleu={
#   "Gulf" : [0,0,0],
#    "Iraqi":[0,0,0],
#   "Levantine":[0,0,0],
#    "Nile_Basin":[0,0,0],
#    "North_Africa":[0,0,0]}

# data = [

#         data_test[0],

#     ]
